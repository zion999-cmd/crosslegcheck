#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºæˆåŠŸç»éªŒçš„ç®€å•æœ‰æ•ˆCNNæ¨¡å‹
å‚è€ƒMNISTçš„æˆåŠŸæ¨¡å¼ï¼Œä½†é€‚é…å‹åŠ›ä¼ æ„Ÿå™¨æ•°æ®
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import os
import sys

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def pressure_to_image(pressure_data):
    """å°†256ç»´å‹åŠ›æ•°æ®è½¬æ¢ä¸º16x16å›¾åƒ"""
    return pressure_data.reshape(16, 16)

def load_data(csv_file):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {csv_file}")
    
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
    except:
        df = pd.read_csv(csv_file, encoding='gbk')
    
    # åˆ†ç¦»æ ‡ç­¾å’Œç‰¹å¾
    labels = df['Label'].values
    features = df.drop('Label', axis=1).values
    
    print(f"   - æ€»æ ·æœ¬æ•°: {len(features)}")
    print(f"   - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    
    # æ ‡ç­¾åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        print(f"   - {label}: {count} æ ·æœ¬")
    
    # æ•°æ®æ ‡å‡†åŒ– (å…³é”®æ”¹è¿›ï¼)
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # è½¬æ¢ä¸ºå›¾åƒæ ¼å¼
    images = np.array([pressure_to_image(row) for row in features_scaled])
    
    # æ ‡ç­¾ç¼–ç 
    le = LabelEncoder()
    encoded_labels = le.fit_transform(labels)
    
    print(f"   - æ ‡ç­¾ç¼–ç : {dict(zip(le.classes_, range(len(le.classes_))))}")
    
    return images, encoded_labels, le, scaler

def create_simple_effective_model():
    """åˆ›å»ºåŸºäºæ‚¨æˆåŠŸç»éªŒçš„ç®€å•æ¨¡å‹"""
    print("ğŸ—ï¸  åˆ›å»ºç®€å•æœ‰æ•ˆæ¨¡å‹...")
    
    model = keras.models.Sequential([
        # è¾“å…¥å±‚ï¼š16x16çš„å‹åŠ›å›¾åƒ
        keras.layers.Flatten(input_shape=(16, 16)),
        
        # ç¬¬ä¸€å±‚ï¼š128ä¸ªç¥ç»å…ƒ (å’Œæ‚¨çš„MNISTä¸€æ ·)
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dropout(0.2),
        
        # ç¬¬äºŒå±‚ï¼š64ä¸ªç¥ç»å…ƒ (é€‚å½“å‡å°‘)
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.2),
        
        # è¾“å‡ºå±‚ï¼š3ä¸ªç±»åˆ« (left, normal, right)
        keras.layers.Dense(3)  # ä¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œè®©SparseCategoricalCrossentropyå¤„ç†
    ])
    
    print(f"   - æ¨¡å‹å‚æ•°: {model.count_params():,}")
    return model

def train_simple_model(data_path='../data/dataset.csv'):
    """è®­ç»ƒç®€å•æœ‰æ•ˆæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€å•æœ‰æ•ˆæ¨¡å‹...\n")
    
    # åŠ è½½æ•°æ®
    images, labels, le, scaler = load_data(data_path)
    
    # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        images, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"   - è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_simple_effective_model()
    
    # ç¼–è¯‘æ¨¡å‹ (å®Œå…¨æŒ‰ç…§æ‚¨çš„æˆåŠŸé…ç½®)
    model.compile(
        optimizer='adam',  # ä½¿ç”¨é»˜è®¤çš„adam
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy']
    )
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print(f"\nğŸ“‹ æ¨¡å‹ç»“æ„:")
    model.summary()
    
    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    
    history = model.fit(
        X_train, y_train,
        epochs=50,  # é€‚ä¸­çš„è®­ç»ƒè½®æ•°
        validation_data=(X_val, y_val),
        batch_size=32,
        verbose=1,
        callbacks=[
            keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
        ]
    )
    
    # è¯„ä¼°æ¨¡å‹
    print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°:")
    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
    
    print(f"   - è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"   - éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
    
    # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    model_dir = '../models'
    os.makedirs(model_dir, exist_ok=True)
    
    model.save(f'{model_dir}/simple_effective_model.keras')
    
    # ä¿å­˜é¢„å¤„ç†å™¨
    import joblib
    joblib.dump(le, f'{model_dir}/simple_label_encoder.pkl')
    joblib.dump(scaler, f'{model_dir}/simple_scaler.pkl')
    
    print(f"   âœ… æ¨¡å‹å·²ä¿å­˜: simple_effective_model.keras")
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    plot_training_history(history)
    
    return model, le, scaler

def plot_training_history(history):
    """å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
    plt.title('æ¨¡å‹å‡†ç¡®ç‡')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
    plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
    plt.title('æ¨¡å‹æŸå¤±')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('../results/simple_training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def predict_simple_model(test_csv, model_path='../models/simple_effective_model.keras'):
    """ä½¿ç”¨ç®€å•æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    print(f"\nğŸ”® ä½¿ç”¨ç®€å•æœ‰æ•ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    
    # åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    model = keras.models.load_model(model_path)
    
    import joblib
    le = joblib.load('../models/simple_label_encoder.pkl')
    scaler = joblib.load('../models/simple_scaler.pkl')
    
    print(f"   - å·²åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        df = pd.read_csv(test_csv, encoding='utf-8')
    except:
        df = pd.read_csv(test_csv, encoding='gbk')
    
    # æ£€æŸ¥åˆ—æ•°ï¼Œå¤„ç†å¤šä½™åˆ—
    if df.shape[1] == 257:
        if 'Label' in df.columns:
            features = df.drop('Label', axis=1).values
        else:
            features = df.iloc[:, 1:].values
    else:
        features = df.values
    
    print(f"   - æµ‹è¯•æ ·æœ¬æ•°: {len(features)}")
    
    # æ•°æ®é¢„å¤„ç†
    features_scaled = scaler.transform(features)
    images = np.array([pressure_to_image(row) for row in features_scaled])
    
    # é¢„æµ‹
    predictions = model.predict(images)
    predicted_classes = np.argmax(predictions, axis=1)
    
    # è·å–é¢„æµ‹æ¦‚ç‡
    probabilities = tf.nn.softmax(predictions).numpy()
    
    # ç±»åˆ«æ˜ å°„
    class_names = le.classes_
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print(f"\nğŸ“‹ é¢„æµ‹ç»“æœ:")
    print(f"   æ ·æœ¬ç¼–å· | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | å„ç±»åˆ«æ¦‚ç‡")
    print(f"   --------|---------|-------|------------------")
    
    correct_count = 0
    for i, (pred_class, prob) in enumerate(zip(predicted_classes, probabilities)):
        predicted_label = class_names[pred_class]
        confidence = prob[pred_class]
        
        # å‡è®¾æµ‹è¯•æ•°æ®éƒ½æ˜¯leftç±»åˆ«
        is_correct = predicted_label == 'left'
        if is_correct:
            correct_count += 1
        
        status = "âœ…" if is_correct else "âŒ"
        
        prob_str = " | ".join([f"{name}:{p:.3f}" for name, p in zip(class_names, prob)])
        print(f"   {i+1:7d} | {predicted_label:7s} | {confidence:.3f} | {prob_str} {status}")
    
    accuracy = correct_count / len(features)
    print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   - æ­£ç¡®é¢„æµ‹: {correct_count}/{len(features)} ({accuracy:.2%})")
    
    # ç±»åˆ«åˆ†å¸ƒ
    pred_counts = np.bincount(predicted_classes, minlength=len(class_names))
    for i, (name, count) in enumerate(zip(class_names, pred_counts)):
        print(f"   - é¢„æµ‹ä¸º{name}: {count} æ ·æœ¬ ({count/len(features):.1%})")
    
    return predictions, predicted_classes

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python simple_effective_cnn.py [train|predict] [æ–‡ä»¶è·¯å¾„]")
        return
    
    command = sys.argv[1]
    
    if command == 'train':
        data_path = sys.argv[2] if len(sys.argv) > 2 else '../data/dataset.csv'
        train_simple_model(data_path)
        
    elif command == 'predict':
        if len(sys.argv) < 3:
            print("è¯·æä¾›æµ‹è¯•æ–‡ä»¶è·¯å¾„")
            return
        test_path = sys.argv[2]
        predict_simple_model(test_path)
        
    else:
        print("æœªçŸ¥å‘½ä»¤ï¼Œè¯·ä½¿ç”¨ 'train' æˆ– 'predict'")

if __name__ == "__main__":
    main()