#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†å¸ƒåˆ†æè„šæœ¬
åˆ†æè®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„å·®å¼‚ï¼Œæ‰¾å‡º13æ¡æ•°æ®è¡¨ç°ä¸ä½³çš„åŸå› 
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from scipy import stats
import warnings

warnings.filterwarnings('ignore')

def pressure_to_image(pressure_data):
    """å°†256ç»´å‹åŠ›æ•°æ®è½¬æ¢ä¸º16x16å›¾åƒ"""
    image = pressure_data.reshape(16, 16).astype(np.float32)
    
    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    if image.max() > 0:
        image = image / image.max()
    
    return image

def load_data(csv_file, label=None):
    """åŠ è½½æ•°æ®"""
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
    except:
        df = pd.read_csv(csv_file, encoding='gbk')
    
    if 'Label' in df.columns:
        labels = df['Label'].values
        features = df.drop('Label', axis=1).values
    else:
        labels = [label] * len(df) if label else ['unknown'] * len(df)
        features = df.values
    
    return features, labels

def calculate_statistics(features, labels, name):
    """è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š {name} æ•°æ®ç»Ÿè®¡:")
    print(f"   - æ ·æœ¬æ•°: {len(features)}")
    print(f"   - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"   - æ•°å€¼èŒƒå›´: [{np.min(features):.2f}, {np.max(features):.2f}]")
    print(f"   - å‡å€¼: {np.mean(features):.2f}")
    print(f"   - æ ‡å‡†å·®: {np.std(features):.2f}")
    print(f"   - ä¸­ä½æ•°: {np.median(features):.2f}")
    
    # é›¶å€¼ç»Ÿè®¡
    zero_ratio = np.sum(features == 0) / features.size
    print(f"   - é›¶å€¼æ¯”ä¾‹: {zero_ratio:.3f} ({zero_ratio*100:.1f}%)")
    
    # ç±»åˆ«åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    print(f"   - ç±»åˆ«åˆ†å¸ƒ:")
    for label, count in zip(unique_labels, counts):
        print(f"     {label}: {count} æ ·æœ¬")
    
    return {
        'min': np.min(features),
        'max': np.max(features),
        'mean': np.mean(features),
        'std': np.std(features),
        'median': np.median(features),
        'zero_ratio': zero_ratio,
        'labels': dict(zip(unique_labels, counts))
    }

def analyze_pressure_patterns(features, labels, name):
    """åˆ†æå‹åŠ›åˆ†å¸ƒæ¨¡å¼"""
    print(f"\nğŸ” {name} å‹åŠ›æ¨¡å¼åˆ†æ:")
    
    # è½¬æ¢ä¸ºå›¾åƒ
    images = np.array([pressure_to_image(row) for row in features])
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å‹åŠ›ä¸­å¿ƒ
    centers_x = []
    centers_y = []
    total_pressures = []
    
    for img in images:
        y, x = np.indices(img.shape)
        total_pressure = np.sum(img)
        
        if total_pressure > 0:
            center_x = np.sum(x * img) / total_pressure
            center_y = np.sum(y * img) / total_pressure
        else:
            center_x = center_y = 8  # ä¸­å¿ƒä½ç½®
        
        centers_x.append(center_x)
        centers_y.append(center_y)
        total_pressures.append(total_pressure)
    
    centers_x = np.array(centers_x)
    centers_y = np.array(centers_y)
    total_pressures = np.array(total_pressures)
    
    print(f"   - å‹åŠ›ä¸­å¿ƒXåæ ‡: {np.mean(centers_x):.2f} Â± {np.std(centers_x):.2f}")
    print(f"   - å‹åŠ›ä¸­å¿ƒYåæ ‡: {np.mean(centers_y):.2f} Â± {np.std(centers_y):.2f}")
    print(f"   - æ€»å‹åŠ›: {np.mean(total_pressures):.2f} Â± {np.std(total_pressures):.2f}")
    
    # åˆ†æå·¦å³åˆ†å¸ƒ
    left_pressure = np.sum(images[:, :, :8], axis=(1, 2))
    right_pressure = np.sum(images[:, :, 8:], axis=(1, 2))
    
    lr_ratio = left_pressure / (left_pressure + right_pressure + 1e-8)
    print(f"   - å·¦å³å‹åŠ›æ¯”ä¾‹: {np.mean(lr_ratio):.3f} Â± {np.std(lr_ratio):.3f}")
    print(f"     (0.5=å¹³è¡¡, <0.5=å³å, >0.5=å·¦å)")
    
    return {
        'center_x': (np.mean(centers_x), np.std(centers_x)),
        'center_y': (np.mean(centers_y), np.std(centers_y)),
        'total_pressure': (np.mean(total_pressures), np.std(total_pressures)),
        'lr_ratio': (np.mean(lr_ratio), np.std(lr_ratio)),
        'images': images
    }

def visualize_comparison(train_features, train_labels, test_features, test_labels):
    """å¯è§†åŒ–è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„å¯¹æ¯”"""
    print(f"\nğŸ¨ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
    
    # è½¬æ¢ä¸ºå›¾åƒ
    train_images = np.array([pressure_to_image(row) for row in train_features])
    test_images = np.array([pressure_to_image(row) for row in test_features])
    
    # 1. æ˜¾ç¤ºæ¯ç±»çš„å¹³å‡å›¾åƒ
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    # è®­ç»ƒæ•°æ®çš„å¹³å‡å›¾åƒ
    le = LabelEncoder()
    all_labels = np.concatenate([train_labels, test_labels])
    le.fit(all_labels)
    
    train_encoded = le.transform(train_labels)
    
    for i, class_name in enumerate(le.classes_):
        if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç±»åˆ«
            # è®­ç»ƒæ•°æ®å¹³å‡
            class_mask = train_encoded == i
            if np.any(class_mask):
                avg_image = np.mean(train_images[class_mask], axis=0)
                axes[0, i].imshow(avg_image, cmap='hot', interpolation='nearest')
                axes[0, i].set_title(f'è®­ç»ƒ-{class_name}å¹³å‡')
                axes[0, i].axis('off')
            
            # æµ‹è¯•æ•°æ®å¹³å‡ (å¦‚æœæ˜¯leftç±»åˆ«)
            if class_name == 'left':
                avg_test = np.mean(test_images, axis=0)
                axes[1, i].imshow(avg_test, cmap='hot', interpolation='nearest')
                axes[1, i].set_title(f'æµ‹è¯•-{class_name}å¹³å‡')
                axes[1, i].axis('off')
    
    # æ˜¾ç¤ºå·®å¼‚
    if 'left' in le.classes_:
        left_idx = list(le.classes_).index('left')
        train_left_mask = train_encoded == left_idx
        if np.any(train_left_mask):
            train_left_avg = np.mean(train_images[train_left_mask], axis=0)
            test_left_avg = np.mean(test_images, axis=0)
            diff = test_left_avg - train_left_avg
            
            axes[1, 3].imshow(diff, cmap='RdBu', interpolation='nearest', vmin=-0.5, vmax=0.5)
            axes[1, 3].set_title('æµ‹è¯•-è®­ç»ƒå·®å¼‚')
            axes[1, 3].axis('off')
            plt.colorbar(axes[1, 3].images[0], ax=axes[1, 3])
    
    plt.tight_layout()
    plt.savefig('data_comparison_images.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    # 2. ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å‹åŠ›å€¼åˆ†å¸ƒ
    axes[0, 0].hist(train_features.flatten(), bins=50, alpha=0.7, label='è®­ç»ƒæ•°æ®', density=True)
    axes[0, 0].hist(test_features.flatten(), bins=50, alpha=0.7, label='æµ‹è¯•æ•°æ®', density=True)
    axes[0, 0].set_title('å‹åŠ›å€¼åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('å‹åŠ›å€¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].legend()
    axes[0, 0].set_yscale('log')
    
    # æ ·æœ¬æ€»å‹åŠ›åˆ†å¸ƒ
    train_totals = np.sum(train_features, axis=1)
    test_totals = np.sum(test_features, axis=1)
    
    axes[0, 1].hist(train_totals, bins=30, alpha=0.7, label='è®­ç»ƒæ•°æ®', density=True)
    axes[0, 1].hist(test_totals, bins=30, alpha=0.7, label='æµ‹è¯•æ•°æ®', density=True)
    axes[0, 1].set_title('æ ·æœ¬æ€»å‹åŠ›åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('æ€»å‹åŠ›')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].legend()
    
    # é›¶å€¼æ¯”ä¾‹åˆ†å¸ƒ
    train_zero_ratios = np.sum(train_features == 0, axis=1) / train_features.shape[1]
    test_zero_ratios = np.sum(test_features == 0, axis=1) / test_features.shape[1]
    
    axes[1, 0].hist(train_zero_ratios, bins=20, alpha=0.7, label='è®­ç»ƒæ•°æ®', density=True)
    axes[1, 0].hist(test_zero_ratios, bins=20, alpha=0.7, label='æµ‹è¯•æ•°æ®', density=True)
    axes[1, 0].set_title('é›¶å€¼æ¯”ä¾‹åˆ†å¸ƒ')
    axes[1, 0].set_xlabel('é›¶å€¼æ¯”ä¾‹')
    axes[1, 0].set_ylabel('å¯†åº¦')
    axes[1, 0].legend()
    
    # æœ€å¤§å€¼åˆ†å¸ƒ
    train_maxes = np.max(train_features, axis=1)
    test_maxes = np.max(test_features, axis=1)
    
    axes[1, 1].hist(train_maxes, bins=30, alpha=0.7, label='è®­ç»ƒæ•°æ®', density=True)
    axes[1, 1].hist(test_maxes, bins=30, alpha=0.7, label='æµ‹è¯•æ•°æ®', density=True)
    axes[1, 1].set_title('æœ€å¤§å‹åŠ›å€¼åˆ†å¸ƒ')
    axes[1, 1].set_xlabel('æœ€å¤§å‹åŠ›å€¼')
    axes[1, 1].set_ylabel('å¯†åº¦')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.savefig('data_distribution_comparison.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜:")
    print(f"   - data_comparison_images.png (å›¾åƒå¯¹æ¯”)")
    print(f"   - data_distribution_comparison.png (åˆ†å¸ƒå¯¹æ¯”)")

def statistical_tests(train_features, test_features):
    """è¿›è¡Œç»Ÿè®¡æ£€éªŒ"""
    print(f"\nğŸ§® ç»Ÿè®¡æ£€éªŒç»“æœ:")
    
    # KSæ£€éªŒ - æ£€éªŒåˆ†å¸ƒæ˜¯å¦ç›¸åŒ
    train_flat = train_features.flatten()
    test_flat = test_features.flatten()
    
    # éšæœºé‡‡æ ·ä»¥é¿å…è®¡ç®—é‡è¿‡å¤§
    if len(train_flat) > 10000:
        train_sample = np.random.choice(train_flat, 10000, replace=False)
    else:
        train_sample = train_flat
        
    if len(test_flat) > 10000:
        test_sample = np.random.choice(test_flat, 10000, replace=False)
    else:
        test_sample = test_flat
    
    ks_stat, ks_p = stats.ks_2samp(train_sample, test_sample)
    print(f"   - KSæ£€éªŒ (åˆ†å¸ƒç›¸ä¼¼æ€§):")
    print(f"     ç»Ÿè®¡é‡: {ks_stat:.4f}, på€¼: {ks_p:.4e}")
    
    if ks_p < 0.01:
        print(f"     âŒ åˆ†å¸ƒæ˜¾è‘—ä¸åŒ (p < 0.01)")
    elif ks_p < 0.05:
        print(f"     âš ï¸  åˆ†å¸ƒå¯èƒ½ä¸åŒ (p < 0.05)")
    else:
        print(f"     âœ… åˆ†å¸ƒç›¸ä¼¼ (p >= 0.05)")
    
    # å‡å€¼æ£€éªŒ
    train_means = np.mean(train_features, axis=1)
    test_means = np.mean(test_features, axis=1)
    
    t_stat, t_p = stats.ttest_ind(train_means, test_means)
    print(f"\n   - Tæ£€éªŒ (å‡å€¼å·®å¼‚):")
    print(f"     ç»Ÿè®¡é‡: {t_stat:.4f}, på€¼: {t_p:.4f}")
    
    if t_p < 0.01:
        print(f"     âŒ å‡å€¼æ˜¾è‘—ä¸åŒ (p < 0.01)")
    elif t_p < 0.05:
        print(f"     âš ï¸  å‡å€¼å¯èƒ½ä¸åŒ (p < 0.05)")
    else:
        print(f"     âœ… å‡å€¼ç›¸ä¼¼ (p >= 0.05)")

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ” å¼€å§‹æ•°æ®åˆ†å¸ƒåˆ†æ...")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_features, train_labels = load_data('dataset.csv')
    test_features, test_labels = load_data('/Users/bx/Desktop/tmp_left.csv', 'left')
    
    # åªåˆ†æè®­ç»ƒæ•°æ®ä¸­çš„leftç±»åˆ«
    train_left_mask = np.array(train_labels) == 'left'
    train_left_features = train_features[train_left_mask]
    train_left_labels = np.array(train_labels)[train_left_mask]
    
    print(f"\nğŸ¯ ä¸“é—¨åˆ†æleftç±»åˆ«:")
    print(f"   - è®­ç»ƒé›†leftæ ·æœ¬: {len(train_left_features)}")
    print(f"   - æµ‹è¯•é›†leftæ ·æœ¬: {len(test_features)}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    train_stats = calculate_statistics(train_left_features, train_left_labels, "è®­ç»ƒé›†(left)")
    test_stats = calculate_statistics(test_features, test_labels, "æµ‹è¯•é›†(left)")
    
    # åˆ†æå‹åŠ›æ¨¡å¼
    train_patterns = analyze_pressure_patterns(train_left_features, train_left_labels, "è®­ç»ƒé›†(left)")
    test_patterns = analyze_pressure_patterns(test_features, test_labels, "æµ‹è¯•é›†(left)")
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“‹ å…³é”®å·®å¼‚å¯¹æ¯”:")
    print(f"   æ•°å€¼èŒƒå›´:")
    print(f"     è®­ç»ƒ: [{train_stats['min']:.1f}, {train_stats['max']:.1f}]")
    print(f"     æµ‹è¯•: [{test_stats['min']:.1f}, {test_stats['max']:.1f}]")
    
    print(f"   å‹åŠ›ä¸­å¿ƒä½ç½®:")
    print(f"     è®­ç»ƒ: ({train_patterns['center_x'][0]:.2f}, {train_patterns['center_y'][0]:.2f})")
    print(f"     æµ‹è¯•: ({test_patterns['center_x'][0]:.2f}, {test_patterns['center_y'][0]:.2f})")
    
    print(f"   å·¦å³å‹åŠ›æ¯”ä¾‹:")
    print(f"     è®­ç»ƒ: {train_patterns['lr_ratio'][0]:.3f} Â± {train_patterns['lr_ratio'][1]:.3f}")
    print(f"     æµ‹è¯•: {test_patterns['lr_ratio'][0]:.3f} Â± {test_patterns['lr_ratio'][1]:.3f}")
    
    # å¯è§†åŒ–å¯¹æ¯”
    visualize_comparison(train_left_features, train_left_labels, test_features, test_labels)
    
    # ç»Ÿè®¡æ£€éªŒ
    statistical_tests(train_left_features, test_features)
    
    # ç»“è®ºå’Œå»ºè®®
    print(f"\nğŸ’¡ åˆ†æç»“è®º:")
    
    # æ•°å€¼èŒƒå›´å·®å¼‚
    range_diff = abs(test_stats['max'] - train_stats['max']) / train_stats['max']
    if range_diff > 0.2:
        print(f"   âŒ æ•°å€¼èŒƒå›´å·®å¼‚è¾ƒå¤§ ({range_diff*100:.1f}%)")
        print(f"      å»ºè®®: æ£€æŸ¥ä¼ æ„Ÿå™¨æ ¡å‡†æˆ–æ•°æ®é‡‡é›†ç¯å¢ƒ")
    
    # å‹åŠ›ä¸­å¿ƒå·®å¼‚
    center_diff = np.sqrt((train_patterns['center_x'][0] - test_patterns['center_x'][0])**2 + 
                         (train_patterns['center_y'][0] - test_patterns['center_y'][0])**2)
    if center_diff > 1.0:
        print(f"   âš ï¸  å‹åŠ›ä¸­å¿ƒåç§»è¾ƒå¤§ ({center_diff:.2f}åƒç´ )")
        print(f"      å»ºè®®: æ£€æŸ¥åå§¿å®šä¹‰ä¸€è‡´æ€§")
    
    # å·¦å³æ¯”ä¾‹å·®å¼‚
    lr_diff = abs(train_patterns['lr_ratio'][0] - test_patterns['lr_ratio'][0])
    if lr_diff > 0.1:
        print(f"   âš ï¸  å·¦å³å‹åŠ›åˆ†å¸ƒå·®å¼‚æ˜æ˜¾ ({lr_diff:.3f})")
        print(f"      å»ºè®®: è®­ç»ƒæ•°æ®å¯èƒ½éœ€è¦æ›´å¤šç±»ä¼¼æ¨¡å¼çš„æ ·æœ¬")
    
    print(f"\nğŸ¯ æ”¹è¿›å»ºè®®:")
    print(f"   1. æ”¶é›†æ›´å¤šä¸æµ‹è¯•æ•°æ®ç›¸ä¼¼çš„è®­ç»ƒæ ·æœ¬")
    print(f"   2. ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§")  
    print(f"   3. æ£€æŸ¥æ•°æ®é‡‡é›†ç¯å¢ƒå’Œæ ‡æ³¨ä¸€è‡´æ€§")
    print(f"   4. è€ƒè™‘ä½¿ç”¨åŸŸé€‚åº”æŠ€æœ¯å¤„ç†åˆ†å¸ƒå·®å¼‚")

if __name__ == "__main__":
    main()