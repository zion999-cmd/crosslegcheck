#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå§¿åˆ†ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬
æ•´åˆäº†æ•°æ®é›†åˆ†æã€æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€æµ‹è¯•é›†éªŒè¯ç­‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import joblib
import warnings
import sys
from pathlib import Path

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PostureEvaluator:
    """åå§¿åˆ†ç±»è¯„ä¼°å™¨"""
    
    def __init__(self, model_type='standard'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('standard' æˆ– 'improved')
        """
        self.model_type = model_type
        self.model = None
        self.scaler = None
        self.pca = None
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        suffix = '_improved' if model_type == 'improved' else ''
        self.model_path = f'model_svm{suffix}.pkl'
        self.scaler_path = f'scaler{suffix}.pkl'
        self.pca_path = f'pca{suffix}.pkl' if model_type == 'improved' else 'pca.pkl'
    
    def load_models(self):
        """åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨"""
        try:
            if Path(self.model_path).exists():
                self.model = joblib.load(self.model_path)
                print(f"âœ… SVMæ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            else:
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            
            if Path(self.scaler_path).exists():
                self.scaler = joblib.load(self.scaler_path)
                print(f"âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {self.scaler_path}")
            else:
                raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {self.scaler_path}")
            
            if Path(self.pca_path).exists():
                self.pca = joblib.load(self.pca_path)
                print(f"âœ… PCAé™ç»´å™¨åŠ è½½æˆåŠŸ: {self.pca_path}")
            else:
                raise FileNotFoundError(f"PCAæ–‡ä»¶ä¸å­˜åœ¨: {self.pca_path}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_data(self, csv_file):
        """åŠ è½½æ•°æ®"""
        try:
            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_file, encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶: {csv_file}")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("æ— æ³•è¯»å–CSVæ–‡ä»¶")
            
            print(f"ğŸ“Š æ•°æ®åŠ è½½æˆåŠŸ:")
            print(f"   - æ€»æ ·æœ¬æ•°: {len(df)}")
            print(f"   - ç‰¹å¾ç»´åº¦: {df.shape[1] - 1}")
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            if 'Label' in df.columns:
                X = df.drop('Label', axis=1).values
                y = df['Label'].values
            else:
                raise ValueError("æ•°æ®å¿…é¡»åŒ…å«Labelåˆ—")
            
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            print(f"   - ç±»åˆ«åˆ†å¸ƒ:")
            for label, count in zip(unique, counts):
                print(f"     {label}: {count} æ ·æœ¬ ({count/len(y)*100:.1f}%)")
            
            return X, y
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None, None
    
    def analyze_dataset(self, csv_file):
        """åˆ†ææ•°æ®é›†"""
        print(f"ğŸ” æ•°æ®é›†åˆ†æ: {csv_file}")
        
        X, y = self.load_data(csv_file)
        if X is None:
            return
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   - ç‰¹å¾å‡å€¼: {np.mean(X):.2f}")
        print(f"   - ç‰¹å¾æ ‡å‡†å·®: {np.std(X):.2f}")
        print(f"   - ç‰¹å¾æœ€å°å€¼: {np.min(X):.2f}")
        print(f"   - ç‰¹å¾æœ€å¤§å€¼: {np.max(X):.2f}")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        zero_features = np.sum(np.all(X == 0, axis=0))
        constant_features = np.sum(np.var(X, axis=0) == 0)
        
        print(f"   - å…¨é›¶ç‰¹å¾: {zero_features}")
        print(f"   - å¸¸æ•°ç‰¹å¾: {constant_features}")
        
        # ç±»åˆ«å¹³è¡¡æ€§
        unique, counts = np.unique(y, return_counts=True)
        max_ratio = np.max(counts) / np.min(counts)
        print(f"   - ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹: {max_ratio:.2f}")
        
        if max_ratio > 2:
            print("   âš ï¸  æ•°æ®ä¸å¹³è¡¡ï¼Œå»ºè®®ä½¿ç”¨å¹³è¡¡æŠ€æœ¯")
        else:
            print("   âœ… æ•°æ®ç›¸å¯¹å¹³è¡¡")
        
        # ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾
        self.plot_data_distribution(X, y, csv_file)
    
    def plot_data_distribution(self, X, y, title_suffix=""):
        """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. ç±»åˆ«åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            axes[0, 0].bar(unique, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
            axes[0, 0].set_title('ç±»åˆ«åˆ†å¸ƒ')
            axes[0, 0].set_xlabel('ç±»åˆ«')
            axes[0, 0].set_ylabel('æ ·æœ¬æ•°')
            
            # 2. ç‰¹å¾åˆ†å¸ƒçƒ­å›¾ï¼ˆå‰100ä¸ªç‰¹å¾ï¼‰
            sample_features = X[:, :100] if X.shape[1] > 100 else X
            im = axes[0, 1].imshow(sample_features[:50].T, cmap='viridis', aspect='auto')
            axes[0, 1].set_title('ç‰¹å¾çƒ­å›¾ (å‰50æ ·æœ¬)')
            axes[0, 1].set_xlabel('æ ·æœ¬')
            axes[0, 1].set_ylabel('ç‰¹å¾')
            plt.colorbar(im, ax=axes[0, 1])
            
            # 3. ç‰¹å¾ç»Ÿè®¡
            feature_means = np.mean(X, axis=0)
            axes[1, 0].plot(feature_means)
            axes[1, 0].set_title('å„ç‰¹å¾å‡å€¼')
            axes[1, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
            axes[1, 0].set_ylabel('å‡å€¼')
            
            # 4. ç±»åˆ«é—´ç‰¹å¾å¯¹æ¯”ï¼ˆä½¿ç”¨å‰10ä¸ªç‰¹å¾çš„å‡å€¼ï¼‰
            for label in unique:
                mask = y == label
                class_means = np.mean(X[mask, :10], axis=0)
                axes[1, 1].plot(class_means, label=f'{label}', marker='o')
            
            axes[1, 1].set_title('ç±»åˆ«é—´ç‰¹å¾å¯¹æ¯” (å‰10ä¸ªç‰¹å¾)')
            axes[1, 1].set_xlabel('ç‰¹å¾ç´¢å¼•')
            axes[1, 1].set_ylabel('å‡å€¼')
            axes[1, 1].legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            filename = f'data_analysis_{title_suffix}.png'.replace('.csv', '').replace(' ', '_')
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   âœ… æ•°æ®åˆ†æå›¾å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"   âš ï¸  æ•°æ®åˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {e}")
    
    def evaluate_on_dataset(self, csv_file):
        """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ¯ æ¨¡å‹è¯„ä¼°: {csv_file}")
        
        # åŠ è½½æ¨¡å‹
        self.load_models()
        
        # åŠ è½½æ•°æ®
        X, y = self.load_data(csv_file)
        if X is None:
            return
        
        # é¢„å¤„ç†æ•°æ®
        X_scaled = self.scaler.transform(X)
        X_pca = self.pca.transform(X_scaled)
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_pca)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(y, y_pred)
        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y, y_pred))
        
        # æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(y, y_pred, csv_file)
        
        # é”™è¯¯åˆ†æ
        self.analyze_errors(y, y_pred)
        
        # æ³›åŒ–èƒ½åŠ›è¯„ä¼°
        self.assess_generalization(accuracy)
        
        return accuracy
    
    def plot_confusion_matrix(self, y_true, y_pred, title_suffix=""):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        try:
            cm = confusion_matrix(y_true, y_pred, labels=['left', 'normal', 'right'])
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['left', 'normal', 'right'],
                       yticklabels=['left', 'normal', 'right'])
            plt.title(f'æ··æ·†çŸ©é˜µ - {title_suffix}')
            plt.ylabel('å®é™…ç±»åˆ«')
            plt.xlabel('é¢„æµ‹ç±»åˆ«')
            
            filename = f'confusion_matrix_{title_suffix}.png'.replace('.csv', '').replace(' ', '_')
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"   âš ï¸  æ··æ·†çŸ©é˜µç»˜åˆ¶å¤±è´¥: {e}")
    
    def analyze_errors(self, y_true, y_pred):
        """åˆ†æé¢„æµ‹é”™è¯¯"""
        errors = np.where(y_true != y_pred)[0]
        
        if len(errors) == 0:
            print(f"\nâœ… å®Œç¾é¢„æµ‹ï¼Œæ²¡æœ‰é”™è¯¯ï¼")
            return
        
        print(f"\nâŒ é”™è¯¯åˆ†æ (å…±{len(errors)}ä¸ªé”™è¯¯):")
        
        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_types = {}
        for idx in errors:
            error_key = f"{y_true[idx]} â†’ {y_pred[idx]}"
            error_types[error_key] = error_types.get(error_key, 0) + 1
        
        # æŒ‰é”™è¯¯æ¬¡æ•°æ’åº
        sorted_errors = sorted(error_types.items(), key=lambda x: x[1], reverse=True)
        
        for error_type, count in sorted_errors:
            percentage = count / len(errors) * 100
            print(f"   {error_type}: {count} æ¬¡ ({percentage:.1f}%)")
        
        # åˆ†ææœ€å¸¸è§çš„é”™è¯¯
        if sorted_errors:
            most_common_error = sorted_errors[0]
            print(f"\nğŸ’¡ æœ€å¸¸è§é”™è¯¯: {most_common_error[0]} (å {most_common_error[1]/len(errors)*100:.1f}%)")
            
            # ç»™å‡ºæ”¹è¿›å»ºè®®
            error_from, error_to = most_common_error[0].split(' â†’ ')
            if error_from in ['left', 'right'] and error_to in ['left', 'right']:
                print("   å»ºè®®: leftå’Œrightç±»åˆ«å®¹æ˜“æ··æ·†ï¼Œå¯èƒ½éœ€è¦æ›´å¤šåŒºåˆ†æ€§ç‰¹å¾")
            elif 'normal' in [error_from, error_to]:
                print("   å»ºè®®: normalç±»åˆ«çš„ç•Œå®šå¯èƒ½éœ€è¦è°ƒæ•´")
    
    def assess_generalization(self, accuracy):
        """è¯„ä¼°æ³›åŒ–èƒ½åŠ›"""
        print(f"\nğŸ” æ³›åŒ–èƒ½åŠ›è¯„ä¼°:")
        
        if accuracy >= 0.99:
            print("   âš ï¸  å‡†ç¡®ç‡è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
            print("   å»ºè®®: ä½¿ç”¨æ›´å¤šç‹¬ç«‹æµ‹è¯•æ•°æ®éªŒè¯")
        elif accuracy >= 0.95:
            print("   âœ… æ³›åŒ–èƒ½åŠ›ä¼˜ç§€")
        elif accuracy >= 0.90:
            print("   âœ… æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
        elif accuracy >= 0.80:
            print("   âš¡ æ³›åŒ–èƒ½åŠ›ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
            print("   å»ºè®®: è°ƒæ•´æ¨¡å‹å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
        else:
            print("   âŒ æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ")
            print("   å»ºè®®: æ£€æŸ¥æ•°æ®è´¨é‡ã€ç‰¹å¾å·¥ç¨‹æˆ–æ¨¡å‹é€‰æ‹©")
    
    def compare_models(self, test_file):
        """æ¯”è¾ƒstandardå’Œimprovedæ¨¡å‹"""
        print(f"ğŸ†š æ¨¡å‹å¯¹æ¯”åˆ†æ")
        
        results = {}
        
        for model_type in ['standard', 'improved']:
            try:
                print(f"\n--- è¯„ä¼° {model_type} æ¨¡å‹ ---")
                evaluator = PostureEvaluator(model_type)
                accuracy = evaluator.evaluate_on_dataset(test_file)
                results[model_type] = accuracy
                
            except Exception as e:
                print(f"âŒ {model_type} æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
                results[model_type] = None
        
        # ç»“æœå¯¹æ¯”
        print(f"\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        for model_type, accuracy in results.items():
            if accuracy is not None:
                print(f"   {model_type}: {accuracy:.4f} ({accuracy*100:.2f}%)")
            else:
                print(f"   {model_type}: è¯„ä¼°å¤±è´¥")
        
        # æ¨èæœ€ä½³æ¨¡å‹
        valid_results = {k: v for k, v in results.items() if v is not None}
        if valid_results:
            best_model = max(valid_results, key=valid_results.get)
            print(f"\nğŸ† æ¨èä½¿ç”¨æ¨¡å‹: {best_model}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python evaluate.py analyze <csv_file>           # æ•°æ®é›†åˆ†æ")
        print("  python evaluate.py test <csv_file>              # æ¨¡å‹æµ‹è¯•")
        print("  python evaluate.py compare <csv_file>           # æ¨¡å‹å¯¹æ¯”")
        print("  python evaluate.py [--improved] <mode> <file>   # ä½¿ç”¨æ”¹è¿›æ¨¡å‹")
        return
    
    # è§£æå‚æ•°
    args = sys.argv[1:]
    model_type = 'standard'
    
    if args[0] == '--improved':
        model_type = 'improved'
        args = args[1:]
    
    if len(args) < 2:
        print("âŒ ç¼ºå°‘å‚æ•°")
        return
    
    mode = args[0]
    csv_file = args[1]
    
    if not Path(csv_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    
    try:
        evaluator = PostureEvaluator(model_type)
        
        if mode == 'analyze':
            # æ•°æ®é›†åˆ†æ
            evaluator.analyze_dataset(csv_file)
            
        elif mode == 'test':
            # æ¨¡å‹æµ‹è¯•
            evaluator.evaluate_on_dataset(csv_file)
            
        elif mode == 'compare':
            # æ¨¡å‹å¯¹æ¯”
            evaluator.compare_models(csv_file)
            
        else:
            print(f"âŒ æœªçŸ¥çš„è¯„ä¼°æ¨¡å¼: {mode}")
            
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")

if __name__ == "__main__":
    main()