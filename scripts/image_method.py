#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
16x16å›¾åƒæ–¹æ³•éªŒè¯ - ä½¿ç”¨scikit-learnå®ç°
å°†256ç»´å‹åŠ›æ•°æ®è½¬æ¢ä¸º16x16å›¾åƒï¼Œç„¶åä½¿ç”¨ä¸åŒçš„ç‰¹å¾æå–æ–¹æ³•
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from scipy import ndimage
import matplotlib.pyplot as plt
import warnings
import joblib

warnings.filterwarnings('ignore')

def pressure_to_image(pressure_data):
    """å°†256ç»´å‹åŠ›æ•°æ®è½¬æ¢ä¸º16x16å›¾åƒ"""
    image = pressure_data.reshape(16, 16).astype(np.float32)
    
    # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
    if image.max() > 0:
        image = image / image.max()
    
    return image

def extract_image_features(image):
    """ä»16x16å›¾åƒä¸­æå–ç‰¹å¾"""
    features = []
    
    # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features.extend([
        np.mean(image),      # å‡å€¼
        np.std(image),       # æ ‡å‡†å·®
        np.max(image),       # æœ€å¤§å€¼
        np.min(image),       # æœ€å°å€¼
        np.median(image),    # ä¸­ä½æ•°
    ])
    
    # 2. å‡ ä½•ç‰¹å¾
    # é‡å¿ƒä½ç½®
    y, x = np.indices(image.shape)
    total_mass = np.sum(image)
    if total_mass > 0:
        center_x = np.sum(x * image) / total_mass
        center_y = np.sum(y * image) / total_mass
    else:
        center_x = center_y = 8  # ä¸­å¿ƒä½ç½®
    
    features.extend([center_x, center_y])
    
    # 3. åŒºåŸŸç‰¹å¾
    # å·¦å³åŠåŒºçš„è´¨é‡åˆ†å¸ƒ
    left_half = np.sum(image[:, :8])
    right_half = np.sum(image[:, 8:])
    top_half = np.sum(image[:8, :])
    bottom_half = np.sum(image[8:, :])
    
    total = left_half + right_half
    if total > 0:
        left_ratio = left_half / total
        right_ratio = right_half / total
    else:
        left_ratio = right_ratio = 0.5
    
    total_v = top_half + bottom_half
    if total_v > 0:
        top_ratio = top_half / total_v
        bottom_ratio = bottom_half / total_v
    else:
        top_ratio = bottom_ratio = 0.5
    
    features.extend([left_ratio, right_ratio, top_ratio, bottom_ratio])
    
    # 4. çº¹ç†ç‰¹å¾ - ä½¿ç”¨ç®€å•çš„æ¢¯åº¦ç»Ÿè®¡
    # æ°´å¹³æ¢¯åº¦
    grad_x = np.abs(np.diff(image, axis=1))
    grad_y = np.abs(np.diff(image, axis=0))
    
    features.extend([
        np.mean(grad_x),     # æ°´å¹³æ¢¯åº¦å‡å€¼
        np.std(grad_x),      # æ°´å¹³æ¢¯åº¦æ ‡å‡†å·®
        np.mean(grad_y),     # å‚ç›´æ¢¯åº¦å‡å€¼
        np.std(grad_y),      # å‚ç›´æ¢¯åº¦æ ‡å‡†å·®
    ])
    
    # 5. å½¢çŠ¶ç‰¹å¾
    # ä¸»è¦å‹åŠ›ç‚¹çš„æ•°é‡ï¼ˆé˜ˆå€¼æ–¹æ³•ï¼‰
    threshold = np.mean(image) + np.std(image)
    high_pressure_points = np.sum(image > threshold)
    
    # è¿é€šåŒºåŸŸæ•°é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    binary_image = image > (np.max(image) * 0.3)
    
    features.extend([
        high_pressure_points,
        np.sum(binary_image),  # æ´»è·ƒåƒç´ æ•°é‡
    ])
    
    # 6. å¯¹ç§°æ€§ç‰¹å¾
    # å·¦å³å¯¹ç§°æ€§
    left_side = image[:, :8]
    right_side = np.fliplr(image[:, 8:])
    symmetry_lr = np.corrcoef(left_side.flatten(), right_side.flatten())[0, 1]
    if np.isnan(symmetry_lr):
        symmetry_lr = 0
    
    features.append(symmetry_lr)
    
    return np.array(features)

def load_and_extract_features(csv_file):
    """åŠ è½½æ•°æ®å¹¶æå–å›¾åƒç‰¹å¾"""
    print(f"ğŸ”„ åŠ è½½æ•°æ®: {csv_file}")
    
    # è¯»å–CSV
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
    except:
        df = pd.read_csv(csv_file, encoding='gbk')
    
    print(f"   - æ ·æœ¬æ•°: {len(df)}")
    
    # åˆ†ç¦»æ ‡ç­¾å’ŒåŸå§‹ç‰¹å¾
    labels = df['Label'].values
    pressure_data = df.drop('Label', axis=1).values
    
    print(f"ğŸ–¼ï¸  è½¬æ¢ä¸º16x16å›¾åƒå¹¶æå–ç‰¹å¾...")
    
    # è½¬æ¢ä¸ºå›¾åƒå¹¶æå–ç‰¹å¾
    image_features = []
    images = []
    
    for i, row in enumerate(pressure_data):
        # è½¬æ¢ä¸º16x16å›¾åƒ
        image = pressure_to_image(row)
        images.append(image)
        
        # æå–å›¾åƒç‰¹å¾
        features = extract_image_features(image)
        image_features.append(features)
        
        if (i + 1) % 100 == 0:
            print(f"   å¤„ç†è¿›åº¦: {i+1}/{len(pressure_data)}")
    
    image_features = np.array(image_features)
    images = np.array(images)
    
    # ç¼–ç æ ‡ç­¾
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)
    
    print(f"âœ… ç‰¹å¾æå–å®Œæˆ:")
    print(f"   - å›¾åƒç‰¹å¾ç»´åº¦: {image_features.shape[1]}")
    print(f"   - ç±»åˆ«æ•°: {len(label_encoder.classes_)}")
    
    return image_features, encoded_labels, label_encoder, images

def visualize_pressure_images(images, labels, label_encoder, num_samples=6):
    """å¯è§†åŒ–å‹åŠ›å›¾åƒ"""
    plt.figure(figsize=(15, 10))
    
    samples_per_class = num_samples // len(label_encoder.classes_)
    plot_idx = 1
    
    for class_idx, class_name in enumerate(label_encoder.classes_):
        # æ‰¾åˆ°è¯¥ç±»çš„æ ·æœ¬
        class_indices = np.where(labels == class_idx)[0]
        
        for i in range(min(samples_per_class, len(class_indices))):
            sample_idx = class_indices[i]
            
            plt.subplot(len(label_encoder.classes_), samples_per_class, plot_idx)
            plt.imshow(images[sample_idx], cmap='viridis')
            plt.title(f'{class_name} - æ ·æœ¬{i+1}')
            plt.colorbar()
            plt.axis('off')
            plot_idx += 1
    
    plt.tight_layout()
    plt.savefig('pressure_images_by_class.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… å‹åŠ›å›¾åƒåˆ†ç±»æ ·ä¾‹å·²ä¿å­˜: pressure_images_by_class.png")

def compare_methods():
    """æ¯”è¾ƒåŸå§‹æ–¹æ³•vså›¾åƒæ–¹æ³•"""
    print("ğŸ”„ å¼€å§‹æ–¹æ³•å¯¹æ¯”å®éªŒ...")
    
    # åŠ è½½åŸå§‹æ•°æ®ï¼ˆSVMæ–¹æ³•ï¼‰
    df = pd.read_csv('dataset.csv', encoding='utf-8')
    labels = df['Label'].values
    original_features = df.drop('Label', axis=1).values
    
    # å›¾åƒæ–¹æ³•ç‰¹å¾æå–
    image_features, encoded_labels, label_encoder, images = load_and_extract_features('dataset.csv')
    
    # æ•°æ®åˆ†å‰²
    X_orig_train, X_orig_test, y_train, y_test = train_test_split(
        original_features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
    )
    
    X_img_train, X_img_test, _, _ = train_test_split(
        image_features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†: {len(X_orig_train)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(X_orig_test)} æ ·æœ¬")
    print(f"   - åŸå§‹ç‰¹å¾ç»´åº¦: {X_orig_train.shape[1]}")
    print(f"   - å›¾åƒç‰¹å¾ç»´åº¦: {X_img_train.shape[1]}")
    
    # å¯è§†åŒ–å‹åŠ›å›¾åƒ
    train_indices = np.arange(len(X_orig_train))
    visualize_pressure_images(images[train_indices], y_train, label_encoder)
    
    results = {}
    
    # 1. åŸå§‹SVMæ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ç”¨PCAï¼‰
    print(f"\nğŸ”„ æµ‹è¯•åŸå§‹SVMæ–¹æ³•...")
    scaler_orig = StandardScaler()
    X_orig_train_scaled = scaler_orig.fit_transform(X_orig_train)
    X_orig_test_scaled = scaler_orig.transform(X_orig_test)
    
    svm_orig = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)
    svm_orig.fit(X_orig_train_scaled, y_train)
    
    orig_pred = svm_orig.predict(X_orig_test_scaled)
    orig_acc = accuracy_score(y_test, orig_pred)
    results['åŸå§‹SVM'] = orig_acc
    
    print(f"   å‡†ç¡®ç‡: {orig_acc:.4f}")
    
    # 2. å›¾åƒç‰¹å¾ + SVM
    print(f"\nğŸ”„ æµ‹è¯•å›¾åƒç‰¹å¾+SVMæ–¹æ³•...")
    scaler_img = StandardScaler()
    X_img_train_scaled = scaler_img.fit_transform(X_img_train)
    X_img_test_scaled = scaler_img.transform(X_img_test)
    
    svm_img = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)
    svm_img.fit(X_img_train_scaled, y_train)
    
    img_pred = svm_img.predict(X_img_test_scaled)
    img_acc = accuracy_score(y_test, img_pred)
    results['å›¾åƒç‰¹å¾+SVM'] = img_acc
    
    print(f"   å‡†ç¡®ç‡: {img_acc:.4f}")
    
    # 3. å›¾åƒç‰¹å¾ + éšæœºæ£®æ—
    print(f"\nğŸ”„ æµ‹è¯•å›¾åƒç‰¹å¾+éšæœºæ£®æ—æ–¹æ³•...")
    rf_img = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_img.fit(X_img_train, y_train)
    
    rf_pred = rf_img.predict(X_img_test)
    rf_acc = accuracy_score(y_test, rf_pred)
    results['å›¾åƒç‰¹å¾+RF'] = rf_acc
    
    print(f"   å‡†ç¡®ç‡: {rf_acc:.4f}")
    
    # ç»“æœå¯¹æ¯”
    print(f"\nğŸ“Š æ–¹æ³•å¯¹æ¯”ç»“æœ:")
    for method, acc in results.items():
        print(f"   {method}: {acc:.4f} ({acc*100:.2f}%)")
    
    best_method = max(results, key=results.get)
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method} ({results[best_method]*100:.2f}%)")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå›¾åƒç‰¹å¾ç‰ˆæœ¬ï¼‰
    if 'SVM' in best_method:
        joblib.dump(svm_img, 'image_svm_model.pkl')
        joblib.dump(scaler_img, 'image_scaler.pkl')
        print(f"âœ… æœ€ä½³SVMæ¨¡å‹å·²ä¿å­˜")
    
    joblib.dump(label_encoder, 'image_label_encoder.pkl')
    
    return results

def predict_with_image_method(csv_file):
    """ä½¿ç”¨å›¾åƒæ–¹æ³•è¿›è¡Œé¢„æµ‹"""
    print(f"ğŸ”® ä½¿ç”¨å›¾åƒæ–¹æ³•é¢„æµ‹: {csv_file}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model = joblib.load('image_svm_model.pkl')
        scaler = joblib.load('image_scaler.pkl')
        label_encoder = joblib.load('image_label_encoder.pkl')
        print(f"âœ… å›¾åƒæ–¹æ³•æ¨¡å‹åŠ è½½æˆåŠŸ")
    except:
        print(f"âŒ è¯·å…ˆè¿è¡Œè®­ç»ƒ: python image_method.py compare")
        return
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
    except:
        df = pd.read_csv(csv_file, encoding='gbk')
    
    if 'Label' in df.columns:
        true_labels = df['Label'].values
        features = df.drop('Label', axis=1).values
        has_labels = True
    else:
        true_labels = None
        features = df.values
        has_labels = False
    
    print(f"   - æ ·æœ¬æ•°: {len(features)}")
    
    # æå–å›¾åƒç‰¹å¾
    print(f"ğŸ–¼ï¸  æå–å›¾åƒç‰¹å¾...")
    image_features = []
    for row in features:
        image = pressure_to_image(row)
        img_features = extract_image_features(image)
        image_features.append(img_features)
    
    image_features = np.array(image_features)
    
    # æ ‡å‡†åŒ–å¹¶é¢„æµ‹
    image_features_scaled = scaler.transform(image_features)
    predictions = model.predict(image_features_scaled)
    predicted_labels = label_encoder.inverse_transform(predictions)
    
    # ç»Ÿè®¡ç»“æœ
    unique_pred, counts_pred = np.unique(predicted_labels, return_counts=True)
    pred_distribution = {label: int(count) for label, count in zip(unique_pred, counts_pred)}
    
    print(f"âœ… å›¾åƒæ–¹æ³•é¢„æµ‹å®Œæˆ:")
    print(f"   - é¢„æµ‹åˆ†å¸ƒ: {pred_distribution}")
    
    if has_labels:
        accuracy = accuracy_score(true_labels, predicted_labels)
        print(f"   - å›¾åƒæ–¹æ³•å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        print(f"\nğŸ“‹ å›¾åƒæ–¹æ³•åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(true_labels, predicted_labels))
        
        # é”™è¯¯åˆ†æ
        errors = np.where(true_labels != predicted_labels)[0]
        if len(errors) > 0:
            print(f"\nâŒ é”™è¯¯é¢„æµ‹ (å…±{len(errors)}ä¸ª):")
            for idx in errors:
                print(f"   æ ·æœ¬{idx}: {true_labels[idx]} â†’ {predicted_labels[idx]}")
        else:
            print(f"\nâœ… å®Œç¾é¢„æµ‹ï¼Œæ²¡æœ‰é”™è¯¯ï¼")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python image_method.py compare             # å¯¹æ¯”åŸå§‹vså›¾åƒæ–¹æ³•")
        print("  python image_method.py predict <csv_file>  # ä½¿ç”¨å›¾åƒæ–¹æ³•é¢„æµ‹")
        sys.exit(1)
    
    mode = sys.argv[1]
    
    if mode == 'compare':
        print("ğŸš€ å¼€å§‹æ–¹æ³•å¯¹æ¯”å®éªŒ...")
        compare_methods()
        print("\nğŸ‰ å®éªŒå®Œæˆï¼")
        
    elif mode == 'predict':
        if len(sys.argv) < 3:
            print("âŒ è¯·æä¾›CSVæ–‡ä»¶è·¯å¾„")
            sys.exit(1)
        
        csv_file = sys.argv[2]
        predict_with_image_method(csv_file)
    
    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")