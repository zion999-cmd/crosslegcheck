#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ•´åˆè„šæœ¬
æ•´åˆæ‰€æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹æ€§èƒ½
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_and_merge_datasets():
    """åŠ è½½å¹¶æ•´åˆæ‰€æœ‰æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½å¹¶æ•´åˆæ•°æ®é›†...")
    
    all_data = []
    
    # æ•°æ®æ–‡ä»¶åˆ—è¡¨
    data_files = [
        ('../data/dataset.csv', 'åŸå§‹è®­ç»ƒæ•°æ®'),
        ('../data/test_dataset.csv', 'æµ‹è¯•æ•°æ®'),
        ('../data/artificial_test_data.csv', 'äººå·¥æµ‹è¯•æ•°æ®')
    ]
    
    for filename, description in data_files:
        try:
            # å°è¯•ä¸åŒç¼–ç 
            try:
                df = pd.read_csv(filename, encoding='utf-8')
            except:
                df = pd.read_csv(filename, encoding='gbk')
            
            print(f"   âœ… {description} ({filename}):")
            print(f"      - æ ·æœ¬æ•°: {len(df)}")
            
            if 'Label' in df.columns:
                label_counts = df['Label'].value_counts()
                print(f"      - æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")
                all_data.append(df)
            else:
                print(f"      - âš ï¸  æ²¡æœ‰Labelåˆ—ï¼Œè·³è¿‡")
                
        except Exception as e:
            print(f"   âŒ æ— æ³•åŠ è½½ {filename}: {e}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_data:
        merged_df = pd.concat(all_data, ignore_index=True)
        print(f"\nğŸ“Š æ•´åˆåçš„æ•°æ®é›†:")
        print(f"   - æ€»æ ·æœ¬æ•°: {len(merged_df)}")
        print(f"   - ç‰¹å¾ç»´åº¦: {merged_df.shape[1] - 1}")  # å‡å»Labelåˆ—
        
        label_counts = merged_df['Label'].value_counts()
        print(f"   - æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")
        
        return merged_df
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†")
        return None

def analyze_merged_data(df):
    """åˆ†ææ•´åˆåçš„æ•°æ®"""
    print(f"\nğŸ” æ•°æ®è´¨é‡åˆ†æ:")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_values = df.isnull().sum().sum()
    print(f"   - ç¼ºå¤±å€¼: {missing_values}")
    
    # æ£€æŸ¥é‡å¤æ ·æœ¬
    duplicates = df.duplicated().sum()
    print(f"   - é‡å¤æ ·æœ¬: {duplicates}")
    
    # ç‰¹å¾ç»Ÿè®¡
    feature_cols = [col for col in df.columns if col != 'Label']
    features = df[feature_cols].values
    
    print(f"   - ç‰¹å¾èŒƒå›´: {features.min():.2f} ~ {features.max():.2f}")
    print(f"   - ç‰¹å¾å‡å€¼: {features.mean():.2f}")
    print(f"   - ç‰¹å¾æ ‡å‡†å·®: {features.std():.2f}")
    
    # ç±»åˆ«å¹³è¡¡æ€§
    label_counts = df['Label'].value_counts()
    total_samples = len(df)
    
    print(f"\nğŸ“Š ç±»åˆ«å¹³è¡¡æ€§:")
    for label, count in label_counts.items():
        percentage = count / total_samples * 100
        print(f"   - {label}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    # å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    
    plt.subplot(1, 2, 1)
    label_counts.plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen'])
    plt.title('æ•´åˆæ•°æ®é›† - æ ‡ç­¾åˆ†å¸ƒ')
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('æ ·æœ¬æ•°')
    plt.xticks(rotation=0)
    
    plt.subplot(1, 2, 2)
    plt.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',
            colors=['skyblue', 'lightcoral', 'lightgreen'])
    plt.title('æ•´åˆæ•°æ®é›† - æ ‡ç­¾æ¯”ä¾‹')
    
    plt.tight_layout()
    plt.savefig('../results/integrated_data_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return df

def remove_duplicates(df):
    """å»é™¤é‡å¤æ ·æœ¬"""
    print(f"\nğŸ§¹ æ•°æ®æ¸…æ´—:")
    original_size = len(df)
    
    # å»é™¤å®Œå…¨é‡å¤çš„è¡Œ
    df_clean = df.drop_duplicates()
    after_dedup = len(df_clean)
    
    print(f"   - åŸå§‹æ ·æœ¬: {original_size}")
    print(f"   - å»é‡å: {after_dedup}")
    print(f"   - ç§»é™¤é‡å¤: {original_size - after_dedup}")
    
    return df_clean

def balance_dataset(df, method='oversample'):
    """å¹³è¡¡æ•°æ®é›†"""
    print(f"\nâš–ï¸  æ•°æ®å¹³è¡¡ (æ–¹æ³•: {method}):")
    
    label_counts = df['Label'].value_counts()
    max_count = label_counts.max()
    min_count = label_counts.min()
    
    print(f"   - æœ€å¤§ç±»åˆ«æ ·æœ¬æ•°: {max_count}")
    print(f"   - æœ€å°ç±»åˆ«æ ·æœ¬æ•°: {min_count}")
    print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {max_count/min_count:.2f}:1")
    
    if method == 'oversample':
        # ä¸Šé‡‡æ ·åˆ°æœ€å¤§ç±»åˆ«çš„æ ·æœ¬æ•°
        balanced_data = []
        
        for label in df['Label'].unique():
            label_data = df[df['Label'] == label]
            current_count = len(label_data)
            
            if current_count < max_count:
                # éšæœºé‡é‡‡æ ·
                additional_samples = max_count - current_count
                resampled = label_data.sample(n=additional_samples, replace=True, random_state=42)
                balanced_data.append(pd.concat([label_data, resampled]))
            else:
                balanced_data.append(label_data)
        
        balanced_df = pd.concat(balanced_data, ignore_index=True)
        
    elif method == 'undersample':
        # ä¸‹é‡‡æ ·åˆ°æœ€å°ç±»åˆ«çš„æ ·æœ¬æ•°
        balanced_data = []
        
        for label in df['Label'].unique():
            label_data = df[df['Label'] == label]
            sampled_data = label_data.sample(n=min_count, random_state=42)
            balanced_data.append(sampled_data)
        
        balanced_df = pd.concat(balanced_data, ignore_index=True)
    
    # æ‰“ä¹±æ•°æ®
    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"   - å¹³è¡¡åæ€»æ ·æœ¬: {len(balanced_df)}")
    balanced_counts = balanced_df['Label'].value_counts()
    for label, count in balanced_counts.items():
        print(f"   - {label}: {count} æ ·æœ¬")
    
    return balanced_df

def save_integrated_dataset(df, filename='../data/integrated_dataset.csv'):
    """ä¿å­˜æ•´åˆåçš„æ•°æ®é›†"""
    print(f"\nğŸ’¾ ä¿å­˜æ•´åˆæ•°æ®é›†: {filename}")
    
    df.to_csv(filename, index=False, encoding='utf-8')
    
    print(f"   âœ… å·²ä¿å­˜ {len(df)} ä¸ªæ ·æœ¬åˆ° {filename}")
    
    # åˆ›å»ºè®­ç»ƒ/éªŒè¯åˆ†å‰²
    features = df.drop('Label', axis=1)
    labels = df['Label']
    
    X_train, X_val, y_train, y_val = train_test_split(
        features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # ä¿å­˜è®­ç»ƒé›†
    train_df = pd.concat([X_train, y_train], axis=1)
    train_df.to_csv('../data/integrated_train.csv', index=False, encoding='utf-8')
    
    # ä¿å­˜éªŒè¯é›†
    val_df = pd.concat([X_val, y_val], axis=1)
    val_df.to_csv('../data/integrated_val.csv', index=False, encoding='utf-8')
    
    print(f"   âœ… è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬ â†’ integrated_train.csv")
    print(f"   âœ… éªŒè¯é›†: {len(val_df)} æ ·æœ¬ â†’ integrated_val.csv")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ•°æ®æ•´åˆæµç¨‹...\n")
    
    # 1. åŠ è½½å¹¶åˆå¹¶æ•°æ®
    merged_df = load_and_merge_datasets()
    if merged_df is None:
        return
    
    # 2. åˆ†ææ•°æ®
    analyzed_df = analyze_merged_data(merged_df)
    
    # 3. æ¸…ç†æ•°æ®
    clean_df = remove_duplicates(analyzed_df)
    
    # 4. æ•°æ®å¹³è¡¡ (å¯é€‰)
    print(f"\né€‰æ‹©æ•°æ®å¹³è¡¡ç­–ç•¥:")
    print(f"   1. ä¸è¿›è¡Œå¹³è¡¡ (ä¿æŒåŸå§‹åˆ†å¸ƒ)")
    print(f"   2. ä¸Šé‡‡æ · (å¢åŠ å°‘æ•°ç±»æ ·æœ¬)")
    print(f"   3. ä¸‹é‡‡æ · (å‡å°‘å¤šæ•°ç±»æ ·æœ¬)")
    
    choice = input("è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤1): ").strip()
    
    if choice == '2':
        final_df = balance_dataset(clean_df, 'oversample')
    elif choice == '3':
        final_df = balance_dataset(clean_df, 'undersample')
    else:
        final_df = clean_df
        print(f"\nğŸ“Š ä¿æŒåŸå§‹æ•°æ®åˆ†å¸ƒ")
    
    # 5. ä¿å­˜ç»“æœ
    save_integrated_dataset(final_df)
    
    print(f"\nğŸ‰ æ•°æ®æ•´åˆå®Œæˆï¼")
    print(f"   - å¯ä»¥ä½¿ç”¨ integrated_dataset.csv è¿›è¡Œè®­ç»ƒ")
    print(f"   - æˆ–ç›´æ¥ä½¿ç”¨ integrated_train.csv å’Œ integrated_val.csv")

if __name__ == "__main__":
    main()