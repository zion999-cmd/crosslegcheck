#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸¦æ•°æ®å¢å¼ºçš„CNNæ¨¡å‹
é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯ç¼“è§£æ ·æœ¬ä¸è¶³é—®é¢˜
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
np.random.seed(42)
tf.random.set_seed(42)

def pressure_to_image(pressure_data):
    """å°†256ç»´å‹åŠ›æ•°æ®è½¬æ¢ä¸º16x16å›¾åƒ"""
    image = pressure_data.reshape(16, 16).astype(np.float32)
    
    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    if image.max() > 0:
        image = image / image.max()
    
    return image

def augment_pressure_image(image, augment_params=None):
    """å¯¹å‹åŠ›å›¾åƒè¿›è¡Œæ•°æ®å¢å¼º
    
    Args:
        image: 16x16çš„å‹åŠ›å›¾åƒ
        augment_params: å¢å¼ºå‚æ•°å­—å…¸
    
    Returns:
        å¢å¼ºåçš„å›¾åƒ
    """
    if augment_params is None:
        augment_params = {
            'noise_std': 0.02,
            'brightness_range': 0.1,
            'rotation_range': 5,
            'shift_range': 1,
            'zoom_range': 0.05,
        }
    
    aug_image = image.copy()
    
    # 1. æ·»åŠ é«˜æ–¯å™ªå£°
    if np.random.random() < 0.7:
        noise = np.random.normal(0, augment_params['noise_std'], image.shape)
        aug_image = aug_image + noise
        aug_image = np.clip(aug_image, 0, 1)
    
    # 2. äº®åº¦è°ƒæ•´
    if np.random.random() < 0.5:
        brightness_factor = 1 + np.random.uniform(-augment_params['brightness_range'], 
                                                 augment_params['brightness_range'])
        aug_image = aug_image * brightness_factor
        aug_image = np.clip(aug_image, 0, 1)
    
    # 3. è½»å¾®æ—‹è½¬ (é€šè¿‡TensorFlowå®ç°)
    if np.random.random() < 0.4:
        angle = np.random.uniform(-augment_params['rotation_range'], 
                                augment_params['rotation_range'])
        aug_image = tf.image.rot90(aug_image, k=int(angle/90)) if abs(angle) > 45 else aug_image
    
    # 4. å¹³ç§»
    if np.random.random() < 0.6:
        shift_x = int(np.random.uniform(-augment_params['shift_range'], 
                                       augment_params['shift_range']))
        shift_y = int(np.random.uniform(-augment_params['shift_range'], 
                                       augment_params['shift_range']))
        
        aug_image = np.roll(aug_image, shift_x, axis=0)
        aug_image = np.roll(aug_image, shift_y, axis=1)
    
    # 5. ç¼©æ”¾ï¼ˆé€šè¿‡è£å‰ªå’Œå¡«å……å®ç°ï¼‰
    if np.random.random() < 0.3:
        zoom_factor = 1 + np.random.uniform(-augment_params['zoom_range'], 
                                          augment_params['zoom_range'])
        
        if zoom_factor < 1:  # ç¼©å° - å‘¨å›´å¡«å……0
            h, w = aug_image.shape
            new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)
            
            if new_h > 0 and new_w > 0:
                # ä»ä¸­å¿ƒè£å‰ª
                start_h = (h - new_h) // 2
                start_w = (w - new_w) // 2
                
                cropped = aug_image[start_h:start_h+new_h, start_w:start_w+new_w]
                
                # ç¼©æ”¾å›åŸå°ºå¯¸
                aug_image = tf.image.resize(cropped[..., None], [h, w]).numpy()[..., 0]
    
    return aug_image

def create_augmented_dataset(X, y, augmentation_factor=3):
    """åˆ›å»ºå¢å¼ºæ•°æ®é›†"""
    print(f"   - åˆ›å»ºå¢å¼ºæ•°æ®é›† (å¢å¼ºå€æ•°: {augmentation_factor}x)...")
    
    X_augmented = []
    y_augmented = []
    
    # æ·»åŠ åŸå§‹æ•°æ®
    for i in range(len(X)):
        X_augmented.append(X[i])
        y_augmented.append(y[i])
    
    # æ·»åŠ å¢å¼ºæ•°æ®
    for i in range(len(X)):
        for _ in range(augmentation_factor):
            aug_image = augment_pressure_image(X[i])
            X_augmented.append(aug_image)
            y_augmented.append(y[i])
    
    # æ‰“ä¹±æ•°æ®
    indices = np.random.permutation(len(X_augmented))
    X_augmented = np.array(X_augmented)[indices]
    y_augmented = np.array(y_augmented)[indices]
    
    print(f"   - åŸå§‹æ•°æ®: {len(X)} æ ·æœ¬")
    print(f"   - å¢å¼ºå: {len(X_augmented)} æ ·æœ¬")
    
    return X_augmented, y_augmented

def load_data(csv_file='../data/dataset.csv'):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {csv_file}")
    
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
    except:
        df = pd.read_csv(csv_file, encoding='gbk')
    
    # åˆ†ç¦»æ ‡ç­¾å’Œç‰¹å¾
    labels = df['Label'].values
    features = df.drop('Label', axis=1).values
    
    print(f"   - æ€»æ ·æœ¬æ•°: {len(features)}")
    print(f"   - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        print(f"   - {label}: {count} æ ·æœ¬")
    
    return features, labels

def create_enhanced_cnn(input_shape=(16, 16, 1), num_classes=3):
    """åˆ›å»ºå¢å¼ºçš„CNNæ¨¡å‹"""
    model = keras.models.Sequential([
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'),
        keras.layers.MaxPooling2D((2, 2)),
        keras.layers.Dropout(0.25),
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        keras.layers.MaxPooling2D((2, 2)),
        keras.layers.Dropout(0.25),
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.GlobalAveragePooling2D(),
        
        # åˆ†ç±»å™¨
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

def train_augmented_cnn(features, labels, model_path='../models/cnn_augmented_model.keras'):
    """è®­ç»ƒå¸¦æ•°æ®å¢å¼ºçš„CNNæ¨¡å‹"""
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒå¢å¼ºCNNæ¨¡å‹...")
    
    # è½¬æ¢ä¸ºå›¾åƒæ ¼å¼
    print(f"   - è½¬æ¢ä¸º16x16å›¾åƒæ ¼å¼...")
    images = np.array([pressure_to_image(row) for row in features])
    images = images[..., np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦
    
    # ç¼–ç æ ‡ç­¾
    le = LabelEncoder()
    encoded_labels = le.fit_transform(labels)
    
    print(f"   - æ ‡ç­¾ç¼–ç : {dict(zip(le.classes_, range(len(le.classes_))))}")
    
    # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        images, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
    )
    
    print(f"   - è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    
    # åˆ›å»ºå¢å¼ºæ•°æ®é›†
    X_train_aug, y_train_aug = create_augmented_dataset(
        X_train.squeeze(), y_train, augmentation_factor=3
    )
    
    # æ·»åŠ é€šé“ç»´åº¦
    X_train_aug = X_train_aug[..., np.newaxis]
    
    # åˆ›å»ºæ¨¡å‹
    model = create_enhanced_cnn()
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print(f"\nğŸ“‹ æ¨¡å‹ç»“æ„:")
    model.summary()
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks_list = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=8,
            min_lr=1e-7,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            model_path,
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    history = model.fit(
        X_train_aug, y_train_aug,
        batch_size=32,
        epochs=100,
        validation_data=(X_val, y_val),
        callbacks=callbacks_list,
        verbose=1
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy']):.4f}")
    print(f"   - æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # å¯è§†åŒ–è®­ç»ƒå†å²
    visualize_training_history(history)
    
    return model, le, history

def visualize_training_history(history):
    """å¯è§†åŒ–è®­ç»ƒå†å²"""
    print(f"\nğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # å‡†ç¡®ç‡
    ax1.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡ (å¸¦æ•°æ®å¢å¼º)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('å‡†ç¡®ç‡')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æŸå¤±
    ax2.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
    ax2.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
    ax2.set_title('æ¨¡å‹æŸå¤± (å¸¦æ•°æ®å¢å¼º)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('æŸå¤±')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../results/cnn_augmented_training_history.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… è®­ç»ƒå†å²å›¾å·²ä¿å­˜: cnn_augmented_training_history.png")

def predict_with_augmented_cnn(test_csv, model_path='../models/cnn_augmented_model.keras', expected_label=None):
    """ä½¿ç”¨å¢å¼ºCNNæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    print(f"\nğŸ”® ä½¿ç”¨å¢å¼ºCNNæ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    
    # åŠ è½½æ¨¡å‹
    model = keras.models.load_model(model_path)
    print(f"   - å·²åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        df = pd.read_csv(test_csv, encoding='utf-8')
    except:
        df = pd.read_csv(test_csv, encoding='gbk')
    
    # æ£€æŸ¥åˆ—æ•°ï¼Œå¦‚æœæœ‰æ ‡ç­¾åˆ—åˆ™åˆ é™¤
    has_label = False
    true_labels = None
    
    if df.shape[1] == 257:
        if 'Label' in df.columns:
            true_labels = df['Label'].values
            features = df.drop('Label', axis=1).values
            has_label = True
        else:
            # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ ‡ç­¾æˆ–ç´¢å¼•
            features = df.iloc[:, 1:].values
    else:
        features = df.values
    
    print(f"   - æµ‹è¯•æ ·æœ¬æ•°: {len(features)}")
    print(f"   - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    
    # è½¬æ¢ä¸ºå›¾åƒæ ¼å¼
    images = np.array([pressure_to_image(row) for row in features])
    images = images[..., np.newaxis]
    
    # é¢„æµ‹
    predictions = model.predict(images)
    predicted_classes = np.argmax(predictions, axis=1)
    
    # ç±»åˆ«æ˜ å°„
    class_names = ['left', 'normal', 'right']
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print(f"\nğŸ“‹ é¢„æµ‹ç»“æœ:")
    if has_label:
        print(f"   æ ·æœ¬ç¼–å· | çœŸå®æ ‡ç­¾ | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | å„ç±»åˆ«æ¦‚ç‡ | ç»“æœ")
        print(f"   --------|---------|---------|-------|-----------|----")
    elif expected_label:
        print(f"   æ ·æœ¬ç¼–å· | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | å„ç±»åˆ«æ¦‚ç‡ | ç»“æœ(æœŸæœ›:{expected_label})")
        print(f"   --------|---------|-------|-----------|----------------")
    else:
        print(f"   æ ·æœ¬ç¼–å· | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | å„ç±»åˆ«æ¦‚ç‡")
        print(f"   --------|---------|-------|------------------")
    
    correct_count = 0
    for i, (pred_class, prob) in enumerate(zip(predicted_classes, predictions)):
        predicted_label = class_names[pred_class]
        confidence = prob[pred_class]
        
        # åˆ¤æ–­æ­£ç¡®æ€§
        if has_label:
            true_label = true_labels[i]
            is_correct = predicted_label == true_label
        elif expected_label:
            is_correct = predicted_label == expected_label
        else:
            is_correct = None
        
        if is_correct is not None:
            if is_correct:
                correct_count += 1
            status = "âœ…" if is_correct else "âŒ"
        else:
            status = ""
        
        prob_str = " | ".join([f"{name}:{p:.3f}" for name, p in zip(class_names, prob)])
        
        if has_label:
            print(f"   {i+1:7d} | {true_labels[i]:7s} | {predicted_label:7s} | {confidence:.3f} | {prob_str} | {status}")
        elif expected_label:
            print(f"   {i+1:7d} | {predicted_label:7s} | {confidence:.3f} | {prob_str} | {status}")
        else:
            print(f"   {i+1:7d} | {predicted_label:7s} | {confidence:.3f} | {prob_str}")
    
    # ç»Ÿè®¡ç»“æœ
    if has_label or expected_label:
        accuracy = correct_count / len(features)
        print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        print(f"   - æ­£ç¡®é¢„æµ‹: {correct_count}/{len(features)} ({accuracy:.2%})")
    else:
        print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    
    # ç±»åˆ«åˆ†å¸ƒ
    pred_counts = np.bincount(predicted_classes, minlength=3)
    for i, (name, count) in enumerate(zip(class_names, pred_counts)):
        print(f"   - é¢„æµ‹ä¸º{name}: {count} æ ·æœ¬ ({count/len(features):.1%})")
    
    return predictions, predicted_classes

def visualize_augmentation_samples():
    """å¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ"""
    print(f"\nğŸ¨ ç”Ÿæˆæ•°æ®å¢å¼ºç¤ºä¾‹...")
    
    # åŠ è½½ä¸€ä¸ªæ ·æœ¬
    features, labels = load_data('../data/dataset.csv')
    
    # æ‰¾ä¸€ä¸ªleftç±»åˆ«çš„æ ·æœ¬
    left_indices = np.where(np.array(labels) == 'left')[0]
    sample_idx = left_indices[0]
    sample_image = pressure_to_image(features[sample_idx])
    
    # ç”Ÿæˆå¢å¼ºæ ·æœ¬
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    
    # åŸå§‹å›¾åƒ
    axes[0, 0].imshow(sample_image, cmap='hot', interpolation='nearest')
    axes[0, 0].set_title('åŸå§‹å›¾åƒ')
    axes[0, 0].axis('off')
    
    # å¢å¼ºæ ·æœ¬
    augment_types = ['å™ªå£°', 'äº®åº¦', 'æ—‹è½¬', 'å¹³ç§»', 'ç¼©æ”¾']
    
    for i in range(4):
        aug_image = augment_pressure_image(sample_image)
        axes[0, i+1].imshow(aug_image, cmap='hot', interpolation='nearest')
        axes[0, i+1].set_title(f'å¢å¼ºæ ·æœ¬ {i+1}')
        axes[0, i+1].axis('off')
    
    # æ˜¾ç¤ºä¸åŒå¢å¼ºç±»å‹çš„æ•ˆæœ
    for i, aug_type in enumerate(augment_types):
        aug_image = augment_pressure_image(sample_image)
        axes[1, i].imshow(aug_image, cmap='hot', interpolation='nearest')
        axes[1, i].set_title(f'{aug_type}å¢å¼º')
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.savefig('../results/data_augmentation_samples.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… æ•°æ®å¢å¼ºç¤ºä¾‹å·²ä¿å­˜: data_augmentation_samples.png")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ğŸ“– ä½¿ç”¨æ–¹æ³•:")
        print("   python cnn_augmented.py train                    - è®­ç»ƒæ¨¡å‹")
        print("   python cnn_augmented.py predict <csv>           - é¢„æµ‹(è‡ªåŠ¨åˆ¤æ–­)")
        print("   python cnn_augmented.py predict <csv> <label>   - é¢„æµ‹(æŒ‡å®šæœŸæœ›æ ‡ç­¾)")
        print("   python cnn_augmented.py visualize               - å¯è§†åŒ–å¢å¼ºæ•ˆæœ")
        return
    
    command = sys.argv[1]
    
    if command == 'train':
        print("ğŸ”¥ è®­ç»ƒå¸¦æ•°æ®å¢å¼ºçš„CNNæ¨¡å‹")
        
        # å¯è§†åŒ–å¢å¼ºæ•ˆæœ
        visualize_augmentation_samples()
        
        # åŠ è½½æ•°æ®å¹¶è®­ç»ƒ
        features, labels = load_data('../data/dataset.csv')
        model, le, history = train_augmented_cnn(features, labels)
        
        print(f"\nğŸ¯ è®­ç»ƒæ€»ç»“:")
        print(f"   - æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(features) * 4} (åŸå§‹ + 3å€å¢å¼º)")
        print(f"   - æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy']):.4f}")
        print(f"   - æ¨¡å‹å‚æ•°é‡: {model.count_params():,}")
        
    elif command == 'predict':
        if len(sys.argv) < 3:
            print("âŒ è¯·æä¾›CSVæ–‡ä»¶è·¯å¾„")
            return
        
        test_csv = sys.argv[2]
        expected_label = sys.argv[3] if len(sys.argv) > 3 else None
        predict_with_augmented_cnn(test_csv, expected_label=expected_label)
        
    elif command == 'visualize':
        visualize_augmentation_samples()
        
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")

if __name__ == "__main__":
    main()