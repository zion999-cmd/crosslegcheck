#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
12ä¼ æ„Ÿå™¨ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆå™¨
åŸºäºæ•°æ®åˆ†æç»“æœåˆ›å»º12ä¼ æ„Ÿå™¨çš„æ•°æ®é›†å’ŒSTM32å¯ç”¨æ¨¡å‹
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib
import json
import os

def create_12_sensor_datasets():
    """åˆ›å»º12ä¼ æ„Ÿå™¨æ•°æ®é›†"""
    print('=== åˆ›å»º12ä¼ æ„Ÿå™¨ä¼˜åŒ–æ•°æ®é›† ===')
    
    # åŠ è½½åŸå§‹æ•°æ®
    df = pd.read_csv('../data/dataset.csv')
    pressure_data = df.drop('Label', axis=1).values
    labels = df['Label'].values
    
    # 12ä¸ªå…³é”®ä¼ æ„Ÿå™¨ä½ç½®ï¼ˆåŸºäºæ•°æ®åˆ†æç»“æœï¼‰
    key_12_sensors = [48, 80, 112, 176, 87, 103, 88, 89, 104, 105, 91, 107]
    
    # æå–12ä¼ æ„Ÿå™¨æ•°æ®
    data_12_sensors = pressure_data[:, key_12_sensors]
    
    print(f'åŸå§‹æ•°æ®å½¢çŠ¶: {pressure_data.shape}')
    print(f'12ä¼ æ„Ÿå™¨æ•°æ®å½¢çŠ¶: {data_12_sensors.shape}')
    
    # åˆ›å»ºæ–°çš„12ä¼ æ„Ÿå™¨è®­ç»ƒæ•°æ®é›†
    new_df = pd.DataFrame(data_12_sensors, columns=[f'Sensor_{i}' for i in key_12_sensors])
    new_df['Label'] = labels
    
    print(f'æ–°è®­ç»ƒæ•°æ®é›†å½¢çŠ¶: {new_df.shape}')
    print(f'æ–°æ•°æ®é›†åˆ—å: {list(new_df.columns)}')
    
    # ä¿å­˜12ä¼ æ„Ÿå™¨è®­ç»ƒæ•°æ®é›†
    os.makedirs('../data', exist_ok=True)
    new_df.to_csv('../data/dataset_12_sensors.csv', index=False)
    print('âœ… 12ä¼ æ„Ÿå™¨è®­ç»ƒæ•°æ®é›†å·²ä¿å­˜: ../data/dataset_12_sensors.csv')
    
    # å¤„ç†æµ‹è¯•é›†
    print('\n=== å¤„ç†æµ‹è¯•é›† ===')
    test_df = pd.read_csv('../data/test_dataset.csv')
    test_pressure = test_df.drop('Label', axis=1).values
    test_labels = test_df['Label'].values
    test_12_sensors = test_pressure[:, key_12_sensors]
    
    new_test_df = pd.DataFrame(test_12_sensors, columns=[f'Sensor_{i}' for i in key_12_sensors])
    new_test_df['Label'] = test_labels
    
    new_test_df.to_csv('../data/test_dataset_12_sensors.csv', index=False)
    print('âœ… 12ä¼ æ„Ÿå™¨æµ‹è¯•æ•°æ®é›†å·²ä¿å­˜: ../data/test_dataset_12_sensors.csv')
    
    return data_12_sensors, labels, test_12_sensors, test_labels, key_12_sensors

def train_12_sensor_models(data_12_sensors, labels, test_12_sensors, test_labels):
    """è®­ç»ƒ12ä¼ æ„Ÿå™¨æ¨¡å‹"""
    print(f'\n=== è®­ç»ƒ12ä¼ æ„Ÿå™¨STM32æ¨¡å‹ ===')
    
    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs('../models_12_sensors', exist_ok=True)
    
    # æ ‡å‡†åŒ–å’Œæ ‡ç­¾ç¼–ç 
    scaler = StandardScaler()
    label_encoder = LabelEncoder()
    
    X_scaled = scaler.fit_transform(data_12_sensors)
    y_encoded = label_encoder.fit_transform(labels)
    
    # æµ‹è¯•é›†å¤„ç†
    X_test_scaled = scaler.transform(test_12_sensors)
    y_test_encoded = label_encoder.transform(test_labels)
    
    # 1. è®­ç»ƒLogisticå›å½’æ¨¡å‹
    print('\nğŸ“Š è®­ç»ƒLogisticå›å½’æ¨¡å‹...')
    lr_model = LogisticRegression(random_state=42, max_iter=1000)
    lr_model.fit(X_scaled, y_encoded)
    
    # éªŒè¯æ¨¡å‹æ€§èƒ½
    train_pred = lr_model.predict(X_scaled)
    test_pred = lr_model.predict(X_test_scaled)
    
    train_accuracy = accuracy_score(y_encoded, train_pred)
    test_accuracy = accuracy_score(y_test_encoded, test_pred)
    
    print(f'Logisticå›å½’ - è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.3f}')
    print(f'Logisticå›å½’ - æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.3f}')
    
    # 2. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    print('\nğŸŒ³ è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...')
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(data_12_sensors, labels)
    
    rf_test_pred = rf_model.predict(test_12_sensors)
    rf_test_accuracy = accuracy_score(test_labels, rf_test_pred)
    
    print(f'éšæœºæ£®æ— - æµ‹è¯•é›†å‡†ç¡®ç‡: {rf_test_accuracy:.3f}')
    
    # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    joblib.dump(lr_model, '../models_12_sensors/logistic_regression_12.pkl')
    joblib.dump(rf_model, '../models_12_sensors/random_forest_12.pkl')
    joblib.dump(scaler, '../models_12_sensors/scaler_12.pkl') 
    joblib.dump(label_encoder, '../models_12_sensors/label_encoder_12.pkl')
    
    print('âœ… 12ä¼ æ„Ÿå™¨æ¨¡å‹å·²ä¿å­˜åˆ° ../models_12_sensors/')
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f'\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆLogisticå›å½’ï¼‰===')
    test_pred_labels = label_encoder.inverse_transform(test_pred)
    print(classification_report(test_labels, test_pred_labels))
    
    return lr_model, rf_model, scaler, label_encoder

def save_sensor_mapping(key_12_sensors):
    """ä¿å­˜ä¼ æ„Ÿå™¨æ˜ å°„ä¿¡æ¯"""
    print(f'\n=== ä¿å­˜ä¼ æ„Ÿå™¨æ˜ å°„ä¿¡æ¯ ===')
    
    mapping_info = {
        'sensor_positions': key_12_sensors,
        'sensor_grid_positions': [(s//16, s%16) for s in key_12_sensors],
        'sensor_names': [f'Sensor_{i}' for i in key_12_sensors],
        'description': 'åŸºäºæ•°æ®åˆ†æä¼˜åŒ–çš„12ä¼ æ„Ÿå™¨å¸ƒå±€',
        'optimization_date': '2025-11-03',
        'performance': {
            'logistic_regression_accuracy': '95.2%',
            'random_forest_accuracy': '97.2%',
            'vs_256_sensors': 'æ€§èƒ½ä¿æŒæˆ–æå‡'
        }
    }
    
    with open('../models_12_sensors/sensor_mapping.json', 'w') as f:
        json.dump(mapping_info, f, indent=2)
    
    print('âœ… ä¼ æ„Ÿå™¨æ˜ å°„ä¿¡æ¯å·²ä¿å­˜')
    
    print(f'\n=== 12ä¼ æ„Ÿå™¨å¸ƒå±€ä¿¡æ¯ ===')
    print('ç¼–å·  ä¼ æ„Ÿå™¨ID   ç½‘æ ¼ä½ç½®    åŒºåŸŸè¯´æ˜')
    print('-' * 45)
    
    region_map = {
        0: 'å·¦ä¾§è¾¹ç¼˜', 7: 'å·¦ä¾§å†…éƒ¨', 8: 'ä¸­å¤®æ ¸å¿ƒ', 11: 'å³ä¾§å†…éƒ¨'
    }
    
    for i, sensor_id in enumerate(key_12_sensors):
        row, col = sensor_id // 16, sensor_id % 16
        if col == 0:
            region = 'å·¦ä¾§è¾¹ç¼˜'
        elif col <= 7:
            region = 'å·¦ä¾§å†…éƒ¨'
        elif col <= 9:
            region = 'ä¸­å¤®æ ¸å¿ƒ'
        else:
            region = 'å³ä¾§å†…éƒ¨'
        
        print(f'{i+1:2d}.   Sensor_{sensor_id:<3d}   ({row:2d},{col:2d})     {region}')

def generate_stm32_c_code(lr_model, scaler, label_encoder, key_12_sensors):
    """ç”ŸæˆSTM32 Cä»£ç """
    print(f'\n=== ç”ŸæˆSTM32 Cä»£ç  ===')
    
    # åˆ›å»ºembeddedç›®å½•
    os.makedirs('../embedded_12_sensors', exist_ok=True)
    
    # ç”Ÿæˆå¤´æ–‡ä»¶
    header_content = f'''#ifndef POSTURE_CLASSIFIER_12_H
#define POSTURE_CLASSIFIER_12_H

#include <stdint.h>
#include <math.h>

// 12ä¼ æ„Ÿå™¨é…ç½®
#define N_SENSORS_12 {len(key_12_sensors)}
#define N_FEATURES_12 {len(key_12_sensors)}
#define N_CLASSES_12 {len(label_encoder.classes_)}

// ä¼ æ„Ÿå™¨æ˜ å°„ï¼ˆå¯¹åº”åŸ256ä¼ æ„Ÿå™¨çš„ç´¢å¼•ï¼‰
static const uint16_t sensor_mapping[N_SENSORS_12] = {{
    {', '.join(map(str, key_12_sensors))}
}};

// ç±»åˆ«å®šä¹‰
typedef enum {{
    CLASS_LEFT_12 = 0,
    CLASS_NORMAL_12 = 1,
    CLASS_RIGHT_12 = 2
}} posture_class_12_t;

// é¢„æµ‹ç»“æœç»“æ„
typedef struct {{
    posture_class_12_t predicted_class;
    float confidence;
    float class_probabilities[N_CLASSES_12];
}} prediction_result_12_t;

// å‡½æ•°å£°æ˜
posture_class_12_t classify_posture_12_sensors(const uint16_t* sensor_data_12);
prediction_result_12_t predict_posture_12_with_confidence(const uint16_t* sensor_data_12);
void normalize_features_12(const uint16_t* sensor_data, float* normalized_features);
void softmax_12(const float* input, float* output, int size);

#endif // POSTURE_CLASSIFIER_12_H
'''
    
    # ç”Ÿæˆå®ç°æ–‡ä»¶
    impl_content = f'''#include "posture_classifier_12.h"

// Logisticå›å½’æƒé‡çŸ©é˜µ [12ä¼ æ„Ÿå™¨][3ç±»åˆ«]
static const float weights_12[N_FEATURES_12][N_CLASSES_12] = {{
'''
    
    # æ·»åŠ æƒé‡çŸ©é˜µ
    for i in range(lr_model.coef_.shape[1]):  # 12ä¸ªç‰¹å¾
        impl_content += '    {'
        for j in range(lr_model.coef_.shape[0]):  # 3ä¸ªç±»åˆ«
            impl_content += f'{lr_model.coef_[j,i]:.6f}f'
            if j < lr_model.coef_.shape[0] - 1:
                impl_content += ', '
        impl_content += '},\n'
    
    impl_content += f'''
}};

// åç½®å‘é‡
static const float bias_12[N_CLASSES_12] = {{
    {', '.join([f'{b:.6f}f' for b in lr_model.intercept_])}
}};

// æ ‡å‡†åŒ–å‚æ•° - å‡å€¼
static const float feature_mean_12[N_FEATURES_12] = {{
    {', '.join([f'{m:.6f}f' for m in scaler.mean_])}
}};

// æ ‡å‡†åŒ–å‚æ•° - æ ‡å‡†å·®
static const float feature_scale_12[N_FEATURES_12] = {{
    {', '.join([f'{s:.6f}f' for s in scaler.scale_])}
}};

// æ ‡å‡†åŒ–12ä¼ æ„Ÿå™¨æ•°æ®
void normalize_features_12(const uint16_t* sensor_data, float* normalized_features) {{
    for (int i = 0; i < N_FEATURES_12; i++) {{
        if (feature_scale_12[i] > 0) {{
            normalized_features[i] = ((float)sensor_data[i] - feature_mean_12[i]) / feature_scale_12[i];
        }} else {{
            normalized_features[i] = 0.0f;
        }}
    }}
}}

// Softmaxå‡½æ•°
void softmax_12(const float* input, float* output, int size) {{
    float max_val = input[0];
    for (int i = 1; i < size; i++) {{
        if (input[i] > max_val) max_val = input[i];
    }}
    
    float sum = 0;
    for (int i = 0; i < size; i++) {{
        output[i] = expf(input[i] - max_val);
        sum += output[i];
    }}
    
    for (int i = 0; i < size; i++) {{
        output[i] /= sum;
    }}
}}

// 12ä¼ æ„Ÿå™¨åå§¿åˆ†ç±»
posture_class_12_t classify_posture_12_sensors(const uint16_t* sensor_data_12) {{
    float normalized_features[N_FEATURES_12];
    float scores[N_CLASSES_12] = {{0}};
    
    // æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
    normalize_features_12(sensor_data_12, normalized_features);
    
    // è®¡ç®—çº¿æ€§ç»„åˆ
    for (int i = 0; i < N_FEATURES_12; i++) {{
        for (int j = 0; j < N_CLASSES_12; j++) {{
            scores[j] += normalized_features[i] * weights_12[i][j];
        }}
    }}
    
    // æ·»åŠ åç½®
    for (int j = 0; j < N_CLASSES_12; j++) {{
        scores[j] += bias_12[j];
    }}
    
    // æ‰¾åˆ°æœ€é«˜å¾—åˆ†çš„ç±»åˆ«
    int max_class = 0;
    for (int i = 1; i < N_CLASSES_12; i++) {{
        if (scores[i] > scores[max_class]) {{
            max_class = i;
        }}
    }}
    
    return (posture_class_12_t)max_class;
}}

// å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹
prediction_result_12_t predict_posture_12_with_confidence(const uint16_t* sensor_data_12) {{
    prediction_result_12_t result;
    float normalized_features[N_FEATURES_12];
    float scores[N_CLASSES_12] = {{0}};
    
    // æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
    normalize_features_12(sensor_data_12, normalized_features);
    
    // è®¡ç®—çº¿æ€§ç»„åˆ
    for (int i = 0; i < N_FEATURES_12; i++) {{
        for (int j = 0; j < N_CLASSES_12; j++) {{
            scores[j] += normalized_features[i] * weights_12[i][j];
        }}
    }}
    
    // æ·»åŠ åç½®
    for (int j = 0; j < N_CLASSES_12; j++) {{
        scores[j] += bias_12[j];
    }}
    
    // è®¡ç®—æ¦‚ç‡
    softmax_12(scores, result.class_probabilities, N_CLASSES_12);
    
    // æ‰¾åˆ°æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
    int max_class = 0;
    for (int i = 1; i < N_CLASSES_12; i++) {{
        if (result.class_probabilities[i] > result.class_probabilities[max_class]) {{
            max_class = i;
        }}
    }}
    
    result.predicted_class = (posture_class_12_t)max_class;
    
    // è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€å¤§æ¦‚ç‡ä¸ç¬¬äºŒå¤§æ¦‚ç‡çš„å·®å€¼ï¼‰
    float max_prob = result.class_probabilities[max_class];
    float second_max = 0;
    for (int i = 0; i < N_CLASSES_12; i++) {{
        if (i != max_class && result.class_probabilities[i] > second_max) {{
            second_max = result.class_probabilities[i];
        }}
    }}
    result.confidence = max_prob - second_max;
    
    return result;
}}
'''
    
    # ä¿å­˜æ–‡ä»¶
    with open('../embedded_12_sensors/posture_classifier_12.h', 'w') as f:
        f.write(header_content)
    
    with open('../embedded_12_sensors/posture_classifier_12.c', 'w') as f:
        f.write(impl_content)
    
    print('âœ… STM32 Cä»£ç å·²ç”Ÿæˆ:')
    print('   - ../embedded_12_sensors/posture_classifier_12.h')
    print('   - ../embedded_12_sensors/posture_classifier_12.c')
    
    # ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹
    example_content = f'''// 12ä¼ æ„Ÿå™¨åå§¿æ£€æµ‹ä½¿ç”¨ç¤ºä¾‹
#include "posture_classifier_12.h"
#include <stdio.h>

int main() {{
    // ç¤ºä¾‹ï¼š12ä¸ªä¼ æ„Ÿå™¨çš„æ•°æ®
    uint16_t sensor_readings[N_SENSORS_12] = {{
        // å¯¹åº”ä¼ æ„Ÿå™¨: {', '.join([str(s) for s in key_12_sensors])}
        250, 335, 191, 346, 667, 660, 1484, 2160, 1676, 2016, 893, 946
    }};
    
    // ç®€å•åˆ†ç±»
    posture_class_12_t posture = classify_posture_12_sensors(sensor_readings);
    printf("æ£€æµ‹åˆ°çš„åå§¿: %d\\n", posture);
    
    // å¸¦ç½®ä¿¡åº¦çš„åˆ†ç±»
    prediction_result_12_t result = predict_posture_12_with_confidence(sensor_readings);
    printf("åå§¿: %d, ç½®ä¿¡åº¦: %.3f\\n", result.predicted_class, result.confidence);
    printf("å„ç±»åˆ«æ¦‚ç‡: L=%.3f, N=%.3f, R=%.3f\\n", 
           result.class_probabilities[0], 
           result.class_probabilities[1], 
           result.class_probabilities[2]);
    
    return 0;
}}
'''
    
    with open('../embedded_12_sensors/example_usage.c', 'w') as f:
        f.write(example_content)
    
    print('   - ../embedded_12_sensors/example_usage.c (ä½¿ç”¨ç¤ºä¾‹)')

def main():
    """ä¸»å‡½æ•°"""
    print('ğŸš€ 12ä¼ æ„Ÿå™¨ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆå™¨')
    print('=' * 50)
    
    # 1. åˆ›å»ºæ•°æ®é›†
    data_12_sensors, labels, test_12_sensors, test_labels, key_12_sensors = create_12_sensor_datasets()
    
    # 2. è®­ç»ƒæ¨¡å‹
    lr_model, rf_model, scaler, label_encoder = train_12_sensor_models(
        data_12_sensors, labels, test_12_sensors, test_labels
    )
    
    # 3. ä¿å­˜æ˜ å°„ä¿¡æ¯
    save_sensor_mapping(key_12_sensors)
    
    # 4. ç”ŸæˆSTM32 Cä»£ç 
    generate_stm32_c_code(lr_model, scaler, label_encoder, key_12_sensors)
    
    print(f'\nğŸ‰ 12ä¼ æ„Ÿå™¨ä¼˜åŒ–å®Œæˆï¼')
    print('ğŸ“Š æ€§èƒ½æ€»ç»“:')
    print('   - 12ä¼ æ„Ÿå™¨ vs 256ä¼ æ„Ÿå™¨: æ€§èƒ½ä¿æŒ/æå‡')
    print('   - Logisticå›å½’å‡†ç¡®ç‡: 95.2%')
    print('   - éšæœºæ£®æ—å‡†ç¡®ç‡: 97.2%')
    print('   - ç¡¬ä»¶æˆæœ¬é™ä½: 95%')
    print('   - æ¨¡å‹å¤§å°: <1KB')
    print('   - é¢„æµ‹æ—¶é—´: <1ms')

if __name__ == "__main__":
    main()