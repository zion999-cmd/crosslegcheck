#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå§¿åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
æ•´åˆäº†åŸºç¡€è®­ç»ƒå’Œæ”¹è¿›è®­ç»ƒåŠŸèƒ½
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PostureTrainer:
    """åå§¿åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, data_file='dataset.csv', mode='standard'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            mode: è®­ç»ƒæ¨¡å¼ ('standard' æˆ– 'improved')
        """
        self.data_file = data_file
        self.mode = mode
        self.model = None
        self.scaler = None
        self.pca = None
        
    def load_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        try:
            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(self.data_file, encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ•°æ®")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("æ— æ³•è¯»å–CSVæ–‡ä»¶")
            
            print(f"ğŸ“Š æ•°æ®åŠ è½½æˆåŠŸ:")
            print(f"   - æ€»æ ·æœ¬æ•°: {len(df)}")
            print(f"   - ç‰¹å¾ç»´åº¦: {df.shape[1] - 1}")
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            if 'Label' in df.columns:
                X = df.drop('Label', axis=1).values
                y = df['Label'].values
            else:
                raise ValueError("æ•°æ®å¿…é¡»åŒ…å«Labelåˆ—")
            
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            print(f"   - ç±»åˆ«åˆ†å¸ƒ:")
            for label, count in zip(unique, counts):
                print(f"     {label}: {count} æ ·æœ¬")
            
            return X, y
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None, None
    
    def preprocess_data(self, X, y, test_size=0.2):
        """æ•°æ®é¢„å¤„ç†"""
        print(f"\nğŸ”„ æ•°æ®é¢„å¤„ç† (æ¨¡å¼: {self.mode})...")
        
        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # PCAé™ç»´
        if self.mode == 'improved':
            # æ”¹è¿›æ¨¡å¼ï¼šä¿ç•™æ›´å¤šæ–¹å·®ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
            n_components = min(50, X_train_scaled.shape[1])
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šåŸå§‹è®¾ç½®
            n_components = 20
            
        self.pca = PCA(n_components=n_components, random_state=42)
        X_train_pca = self.pca.fit_transform(X_train_scaled)
        X_test_pca = self.pca.transform(X_test_scaled)
        
        explained_variance = np.sum(self.pca.explained_variance_ratio_)
        print(f"   âœ… PCAé™ç»´: {X.shape[1]} â†’ {n_components} ç»´")
        print(f"   âœ… ä¿ç•™æ–¹å·®: {explained_variance:.3f} ({explained_variance*100:.1f}%)")
        
        return X_train_pca, X_test_pca, y_train, y_test
    
    def train_model(self, X_train, y_train):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸ¤– æ¨¡å‹è®­ç»ƒ (æ¨¡å¼: {self.mode})...")
        
        if self.mode == 'improved':
            # æ”¹è¿›æ¨¡å¼ï¼šç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°
            print("   ğŸ” ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°...")
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
                'kernel': ['rbf', 'poly']
            }
            
            svm = SVC(random_state=42)
            grid_search = GridSearchCV(
                svm, param_grid, cv=5, scoring='accuracy',
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train, y_train)
            
            self.model = grid_search.best_estimator_
            print(f"   âœ… æœ€ä¼˜å‚æ•°: {grid_search.best_params_}")
            print(f"   âœ… äº¤å‰éªŒè¯å¾—åˆ†: {grid_search.best_score_:.4f}")
            
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šå›ºå®šå‚æ•°
            self.model = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)
            self.model.fit(X_train, y_train)
            print("   âœ… ä½¿ç”¨å›ºå®šå‚æ•°è®­ç»ƒå®Œæˆ")
        
        return self.model
    
    def evaluate_model(self, X_train, X_test, y_train, y_test):
        """è¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°...")
        
        # è®­ç»ƒé›†è¯„ä¼°
        y_train_pred = self.model.predict(X_train)
        train_accuracy = accuracy_score(y_train, y_train_pred)
        
        # æµ‹è¯•é›†è¯„ä¼°
        y_test_pred = self.model.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        print(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
        print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹
        overfitting = train_accuracy - test_accuracy
        print(f"   è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f}")
        
        if overfitting > 0.1:
            print("   âš ï¸  æ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆ")
        elif overfitting > 0.05:
            print("   âš¡ è½»å¾®è¿‡æ‹Ÿåˆ")
        else:
            print("   âœ… æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_test_pred))
        
        # æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(y_test, y_test_pred)
        
        return train_accuracy, test_accuracy
    
    def plot_confusion_matrix(self, y_true, y_pred):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        try:
            cm = confusion_matrix(y_true, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['left', 'normal', 'right'],
                       yticklabels=['left', 'normal', 'right'])
            plt.title('æ··æ·†çŸ©é˜µ')
            plt.ylabel('å®é™…ç±»åˆ«')
            plt.xlabel('é¢„æµ‹ç±»åˆ«')
            
            filename = f'confusion_matrix_{self.mode}.png'
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"   âš ï¸  æ··æ·†çŸ©é˜µç»˜åˆ¶å¤±è´¥: {e}")
    
    def save_models(self):
        """ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨"""
        print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        suffix = '_improved' if self.mode == 'improved' else ''
        
        try:
            # ä¿å­˜æ¨¡å‹
            model_file = f'model_svm{suffix}.pkl'
            joblib.dump(self.model, model_file)
            print(f"   âœ… SVMæ¨¡å‹å·²ä¿å­˜: {model_file}")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            scaler_file = f'scaler{suffix}.pkl'
            joblib.dump(self.scaler, scaler_file)
            print(f"   âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_file}")
            
            # ä¿å­˜PCA
            pca_file = f'pca{suffix}.pkl'
            if self.mode == 'improved':
                joblib.dump(self.pca, pca_file)
                print(f"   âœ… PCAé™ç»´å™¨å·²ä¿å­˜: {pca_file}")
            else:
                joblib.dump(self.pca, 'pca.pkl')
                print(f"   âœ… PCAé™ç»´å™¨å·²ä¿å­˜: pca.pkl")
                
        except Exception as e:
            print(f"   âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def train_full_pipeline(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹åå§¿åˆ†ç±»æ¨¡å‹è®­ç»ƒ (æ¨¡å¼: {self.mode})...")
        
        # 1. åŠ è½½æ•°æ®
        X, y = self.load_data()
        if X is None:
            return False
        
        # 2. é¢„å¤„ç†
        X_train, X_test, y_train, y_test = self.preprocess_data(X, y)
        
        # 3. è®­ç»ƒæ¨¡å‹
        self.train_model(X_train, y_train)
        
        # 4. è¯„ä¼°æ¨¡å‹
        self.evaluate_model(X_train, X_test, y_train, y_test)
        
        # 5. ä¿å­˜æ¨¡å‹
        self.save_models()
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        return True

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    mode = 'standard'
    data_file = 'dataset.csv'
    
    if len(sys.argv) > 1:
        if sys.argv[1] in ['standard', 'improved']:
            mode = sys.argv[1]
        else:
            print("ä½¿ç”¨æ–¹æ³•: python train.py [standard|improved] [data_file]")
            return
    
    if len(sys.argv) > 2:
        data_file = sys.argv[2]
    
    print(f"è®­ç»ƒæ¨¡å¼: {mode}")
    print(f"æ•°æ®æ–‡ä»¶: {data_file}")
    
    # å¼€å§‹è®­ç»ƒ
    trainer = PostureTrainer(data_file=data_file, mode=mode)
    success = trainer.train_full_pipeline()
    
    if success:
        print(f"\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        if mode == 'improved':
            print("ğŸ’¡ å»ºè®®ä½¿ç”¨ python evaluate.py è¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°")
    else:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥!")

if __name__ == "__main__":
    main()