#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
STM32åµŒå…¥å¼æ¨¡å‹è®¾è®¡
ä¸“ä¸ºSTM32H750è®¾è®¡çš„è½»é‡çº§åˆ†ç±»æ¨¡å‹
"""

import numpy as np
import pandas as pd
import sys
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pickle
import joblib
import json
import time

class STM32ModelDesigner:
    """STM32åµŒå…¥å¼æ¨¡å‹è®¾è®¡å™¨"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.label_encoder = None
        self.feature_extractors = {}
        
    def extract_statistical_features(self, pressure_data):
        """æå–ç»Ÿè®¡ç‰¹å¾ - é€‚åˆåµŒå…¥å¼è®¡ç®—"""
        features = []
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾ (8ä¸ª)
        features.extend([
            np.mean(pressure_data),      # å¹³å‡å€¼
            np.std(pressure_data),       # æ ‡å‡†å·®
            np.min(pressure_data),       # æœ€å°å€¼
            np.max(pressure_data),       # æœ€å¤§å€¼
            np.median(pressure_data),    # ä¸­ä½æ•°
            np.sum(pressure_data),       # æ€»å’Œ
            np.count_nonzero(pressure_data),  # éé›¶ç‚¹æ•°é‡
            np.max(pressure_data) - np.min(pressure_data)  # æå·®
        ])
        
        # åˆ†å¸ƒç‰¹å¾ (4ä¸ª)
        q25, q75 = np.percentile(pressure_data, [25, 75])
        features.extend([
            q25,                         # 25%åˆ†ä½æ•°
            q75,                         # 75%åˆ†ä½æ•°
            q75 - q25,                   # å››åˆ†ä½è·
            np.mean(pressure_data > np.mean(pressure_data))  # è¶…è¿‡å‡å€¼çš„æ¯”ä¾‹
        ])
        
        return np.array(features)
    
    def extract_spatial_features(self, pressure_data):
        """æå–ç©ºé—´ç‰¹å¾ - åŸºäº16x16å›¾åƒçš„ç©ºé—´åˆ†å¸ƒ"""
        # é‡å¡‘ä¸º16x16å›¾åƒ
        image = pressure_data.reshape(16, 16)
        features = []
        
        # é‡å¿ƒè®¡ç®— (2ä¸ªç‰¹å¾)
        total_pressure = np.sum(image)
        if total_pressure > 0:
            # è®¡ç®—è´¨å¿ƒ
            y_indices, x_indices = np.meshgrid(range(16), range(16), indexing='ij')
            center_x = np.sum(x_indices * image) / total_pressure
            center_y = np.sum(y_indices * image) / total_pressure
        else:
            center_x = center_y = 8.0  # ä¸­å¿ƒä½ç½®
        
        features.extend([center_x, center_y])
        
        # åŒºåŸŸå‹åŠ›åˆ†å¸ƒ (4ä¸ªç‰¹å¾)
        left_pressure = np.sum(image[:, :8])      # å·¦åŠéƒ¨åˆ†
        right_pressure = np.sum(image[:, 8:])     # å³åŠéƒ¨åˆ†
        top_pressure = np.sum(image[:8, :])       # ä¸ŠåŠéƒ¨åˆ†
        bottom_pressure = np.sum(image[8:, :])    # ä¸‹åŠéƒ¨åˆ†
        
        if total_pressure > 0:
            features.extend([
                left_pressure / total_pressure,    # å·¦ä¾§å‹åŠ›æ¯”ä¾‹
                right_pressure / total_pressure,   # å³ä¾§å‹åŠ›æ¯”ä¾‹
                top_pressure / total_pressure,     # ä¸Šéƒ¨å‹åŠ›æ¯”ä¾‹
                bottom_pressure / total_pressure   # ä¸‹éƒ¨å‹åŠ›æ¯”ä¾‹
            ])
        else:
            features.extend([0.25, 0.25, 0.25, 0.25])
        
        # å¯¹ç§°æ€§ç‰¹å¾ (2ä¸ªç‰¹å¾)
        left_right_ratio = left_pressure / (right_pressure + 1e-6)
        top_bottom_ratio = top_pressure / (bottom_pressure + 1e-6)
        features.extend([left_right_ratio, top_bottom_ratio])
        
        return np.array(features)
    
    def extract_peak_features(self, pressure_data):
        """æå–å³°å€¼ç‰¹å¾"""
        features = []
        
        # å³°å€¼ç›¸å…³ (4ä¸ªç‰¹å¾)
        sorted_data = np.sort(pressure_data)[::-1]  # é™åºæ’åˆ—
        top_5_avg = np.mean(sorted_data[:5])        # å‰5ä¸ªæœ€å¤§å€¼å¹³å‡
        top_10_avg = np.mean(sorted_data[:10])      # å‰10ä¸ªæœ€å¤§å€¼å¹³å‡
        
        features.extend([
            top_5_avg,
            top_10_avg,
            top_5_avg / (np.mean(pressure_data) + 1e-6),  # å³°å€¼ä¸å‡å€¼æ¯”
            np.max(pressure_data) / (top_5_avg + 1e-6)    # æœ€å¤§å€¼ä¸top5æ¯”
        ])
        
        return np.array(features)
    
    def extract_all_features(self, pressure_data):
        """æå–æ‰€æœ‰ç‰¹å¾"""
        stat_features = self.extract_statistical_features(pressure_data)
        spatial_features = self.extract_spatial_features(pressure_data)
        peak_features = self.extract_peak_features(pressure_data)
        
        return np.concatenate([stat_features, spatial_features, peak_features])
    
    def load_data(self, csv_file='../data/dataset.csv'):
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {csv_file}")
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except:
            df = pd.read_csv(csv_file, encoding='gbk')
        
        # åˆ†ç¦»æ ‡ç­¾å’Œç‰¹å¾
        labels = df['Label'].values
        pressure_data = df.drop('Label', axis=1).values
        
        print(f"   - æ€»æ ·æœ¬æ•°: {len(pressure_data)}")
        print(f"   - åŸå§‹ç‰¹å¾ç»´åº¦: {pressure_data.shape[1]}")
        
        # æå–åµŒå…¥å¼å‹å¥½çš„ç‰¹å¾
        print(f"   - æå–åµŒå…¥å¼ç‰¹å¾...")
        features = []
        for i, data in enumerate(pressure_data):
            if i % 1000 == 0:
                print(f"     å¤„ç†è¿›åº¦: {i}/{len(pressure_data)}")
            feature_vector = self.extract_all_features(data)
            features.append(feature_vector)
        
        features = np.array(features)
        print(f"   - æå–ç‰¹å¾ç»´åº¦: {features.shape[1]}")
        
        return features, labels
    
    def load_test_data(self, csv_file):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {csv_file}")
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except:
            df = pd.read_csv(csv_file, encoding='gbk')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Labelåˆ—
        if 'Label' in df.columns:
            labels = df['Label'].values
            pressure_data = df.drop('Label', axis=1).values
        else:
            labels = None
            pressure_data = df.values
        
        print(f"   - æ€»æ ·æœ¬æ•°: {len(pressure_data)}")
        print(f"   - åŸå§‹ç‰¹å¾ç»´åº¦: {pressure_data.shape[1]}")
        print(f"   - æœ‰æ ‡ç­¾: {'æ˜¯' if labels is not None else 'å¦'}")
        
        # æå–åµŒå…¥å¼å‹å¥½çš„ç‰¹å¾
        print(f"   - æå–åµŒå…¥å¼ç‰¹å¾...")
        features = []
        for i, data in enumerate(pressure_data):
            if i % 100 == 0 and len(pressure_data) > 100:
                print(f"     å¤„ç†è¿›åº¦: {i}/{len(pressure_data)}")
            feature_vector = self.extract_all_features(data)
            features.append(feature_vector)
        
        features = np.array(features)
        print(f"   - æå–ç‰¹å¾ç»´åº¦: {features.shape[1]}")
        
        return features, labels
    
    def train_lightweight_models(self, features, labels):
        """è®­ç»ƒå¤šä¸ªè½»é‡çº§æ¨¡å‹"""
        print(f"\nğŸ‹ï¸ è®­ç»ƒè½»é‡çº§æ¨¡å‹...")
        
        # ç¼–ç æ ‡ç­¾
        self.label_encoder = LabelEncoder()
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['standard'] = scaler
        
        # å®šä¹‰å€™é€‰æ¨¡å‹
        candidate_models = {
            'decision_tree': DecisionTreeClassifier(
                max_depth=8, 
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42
            ),
            'random_forest_small': RandomForestClassifier(
                n_estimators=10,
                max_depth=6,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42
            ),
            'logistic_regression': LogisticRegression(
                random_state=42,
                max_iter=1000
            ),
            'knn_3': KNeighborsClassifier(n_neighbors=3),
            'knn_5': KNeighborsClassifier(n_neighbors=5),
        }
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
        results = {}
        
        for name, model in candidate_models.items():
            print(f"\nğŸ“Š è®­ç»ƒ {name}...")
            
            # é€‰æ‹©åˆé€‚çš„æ•°æ®ï¼ˆKNNå’ŒLogisticå›å½’éœ€è¦æ ‡å‡†åŒ–ï¼‰
            if name in ['logistic_regression', 'knn_3', 'knn_5']:
                X_train_use = X_train_scaled
                X_test_use = X_test_scaled
            else:
                X_train_use = X_train
                X_test_use = X_test
            
            # è®­ç»ƒ
            start_time = time.time()
            model.fit(X_train_use, y_train)
            train_time = time.time() - start_time
            
            # é¢„æµ‹
            start_time = time.time()
            y_pred = model.predict(X_test_use)
            predict_time = time.time() - start_time
            
            # è¯„ä¼°
            accuracy = accuracy_score(y_test, y_pred)
            
            # ä¼°ç®—æ¨¡å‹å¤§å°
            model_size = self.estimate_model_size(model, name)
            
            results[name] = {
                'model': model,
                'accuracy': accuracy,
                'train_time': train_time,
                'predict_time': predict_time,
                'model_size_kb': model_size,
                'predictions': y_pred
            }
            
            print(f"   âœ… å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {train_time:.2f}s")
            print(f"   ğŸš€ é¢„æµ‹æ—¶é—´: {predict_time*1000:.2f}ms")
            print(f"   ğŸ’¾ ä¼°è®¡å¤§å°: {model_size:.1f} KB")
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        for name, result in results.items():
            print(f"\n{name.upper()}:")
            print(classification_report(y_test, result['predictions'], 
                                      target_names=self.label_encoder.classes_))
        
        self.models = {name: result['model'] for name, result in results.items()}
        return results
    
    def estimate_model_size(self, model, model_name):
        """ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆKBï¼‰"""
        if model_name == 'decision_tree':
            # å†³ç­–æ ‘å¤§å°ä¼°ç®—
            tree = model.tree_
            n_nodes = tree.node_count
            # æ¯ä¸ªèŠ‚ç‚¹å¤§çº¦éœ€è¦ï¼šç‰¹å¾ç´¢å¼•(4å­—èŠ‚) + é˜ˆå€¼(4å­—èŠ‚) + å·¦å³å­èŠ‚ç‚¹(8å­—èŠ‚) = 16å­—èŠ‚
            return n_nodes * 16 / 1024
        
        elif 'random_forest' in model_name:
            # éšæœºæ£®æ—å¤§å°ä¼°ç®—
            total_nodes = sum(tree.tree_.node_count for tree in model.estimators_)
            return total_nodes * 16 / 1024
        
        elif model_name == 'logistic_regression':
            # é€»è¾‘å›å½’ï¼šæƒé‡çŸ©é˜µå¤§å°
            n_features = model.coef_.shape[1]
            n_classes = model.coef_.shape[0]
            return (n_features * n_classes + n_classes) * 4 / 1024  # 4å­—èŠ‚/float
        
        elif 'knn' in model_name:
            # KNNï¼šå­˜å‚¨è®­ç»ƒæ•°æ®
            n_samples, n_features = model._fit_X.shape
            return n_samples * n_features * 4 / 1024
        
        else:
            return 0
    
    def generate_c_code(self, model_name='decision_tree', output_dir='../embedded/'):
        """ç”ŸæˆCä»£ç ç”¨äºSTM32"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        if model_name not in self.models:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return
        
        model = self.models[model_name]
        
        if model_name == 'decision_tree':
            self.generate_decision_tree_c_code(model, output_dir)
        elif model_name == 'logistic_regression':
            self.generate_logistic_regression_c_code(model, output_dir)
        else:
            print(f"âŒ æš‚ä¸æ”¯æŒ {model_name} çš„Cä»£ç ç”Ÿæˆ")
    
    def generate_decision_tree_c_code(self, model, output_dir):
        """ç”Ÿæˆå†³ç­–æ ‘çš„Cä»£ç """
        tree = model.tree_
        
        # ç”Ÿæˆå¤´æ–‡ä»¶
        header_content = f"""
#ifndef PRESSURE_CLASSIFIER_H
#define PRESSURE_CLASSIFIER_H

#include <stdint.h>

// ç‰¹å¾æ•°é‡
#define N_FEATURES {tree.n_features}

// ç±»åˆ«æ•°é‡
#define N_CLASSES {tree.n_classes[0]}

// èŠ‚ç‚¹æ•°é‡
#define N_NODES {tree.node_count}

// ç±»åˆ«æ ‡ç­¾
typedef enum {{
    CLASS_LEFT = 0,
    CLASS_NORMAL = 1,
    CLASS_RIGHT = 2
}} class_t;

// å†³ç­–æ ‘èŠ‚ç‚¹ç»“æ„
typedef struct {{
    int16_t feature;        // ç‰¹å¾ç´¢å¼• (-1è¡¨ç¤ºå¶å­èŠ‚ç‚¹)
    float threshold;        // é˜ˆå€¼
    int16_t left_child;     // å·¦å­èŠ‚ç‚¹ç´¢å¼•
    int16_t right_child;    // å³å­èŠ‚ç‚¹ç´¢å¼•
    int16_t class_id;       // ç±»åˆ«IDï¼ˆä»…å¶å­èŠ‚ç‚¹æœ‰æ•ˆï¼‰
}} tree_node_t;

// å‡½æ•°å£°æ˜
float extract_statistical_features(const uint16_t* pressure_data, float* features);
float extract_spatial_features(const uint16_t* pressure_data, float* features);
float extract_peak_features(const uint16_t* pressure_data, float* features);
void extract_all_features(const uint16_t* pressure_data, float* features);
class_t classify_pressure_data(const uint16_t* pressure_data);
int predict_with_confidence(const uint16_t* pressure_data, float* confidence);

#endif // PRESSURE_CLASSIFIER_H
"""
        
        # ç”ŸæˆCå®ç°æ–‡ä»¶
        c_content = f"""
#include "pressure_classifier.h"
#include <math.h>
#include <string.h>

// å†³ç­–æ ‘èŠ‚ç‚¹æ•°æ®
static const tree_node_t tree_nodes[N_NODES] = {{
"""
        
        # ç”ŸæˆèŠ‚ç‚¹æ•°æ®
        for i in range(tree.node_count):
            feature = tree.feature[i]
            threshold = tree.threshold[i]
            left_child = tree.children_left[i]
            right_child = tree.children_right[i]
            
            if feature == -2:  # å¶å­èŠ‚ç‚¹
                class_id = np.argmax(tree.value[i][0])
                c_content += f"    {{-1, 0.0f, -1, -1, {class_id}}}, // Node {i} (leaf)\n"
            else:
                c_content += f"    {{{feature}, {threshold:.6f}f, {left_child}, {right_child}, -1}}, // Node {i}\n"
        
        c_content += f"""
}};

// æå–ç»Ÿè®¡ç‰¹å¾
void extract_statistical_features(const uint16_t* pressure_data, float* features) {{
    float sum = 0, sum_sq = 0;
    uint16_t min_val = 65535, max_val = 0;
    uint16_t non_zero_count = 0;
    
    // è®¡ç®—åŸºç¡€ç»Ÿè®¡é‡
    for (int i = 0; i < 256; i++) {{
        uint16_t val = pressure_data[i];
        sum += val;
        sum_sq += val * val;
        if (val < min_val) min_val = val;
        if (val > max_val) max_val = val;
        if (val > 0) non_zero_count++;
    }}
    
    float mean = sum / 256.0f;
    float variance = (sum_sq / 256.0f) - (mean * mean);
    float std_dev = sqrtf(variance);
    
    features[0] = mean;                    // å¹³å‡å€¼
    features[1] = std_dev;                 // æ ‡å‡†å·®
    features[2] = min_val;                 // æœ€å°å€¼
    features[3] = max_val;                 // æœ€å¤§å€¼
    features[4] = sum;                     // æ€»å’Œ
    features[5] = non_zero_count;          // éé›¶ç‚¹æ•°é‡
    features[6] = max_val - min_val;       // æå·®
    
    // è®¡ç®—ä¸­ä½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    features[7] = mean;  // ç”¨å‡å€¼è¿‘ä¼¼ä¸­ä½æ•°
}}

// æå–ç©ºé—´ç‰¹å¾
void extract_spatial_features(const uint16_t* pressure_data, float* features) {{
    float total_pressure = 0;
    float center_x = 0, center_y = 0;
    
    // è®¡ç®—æ€»å‹åŠ›å’Œé‡å¿ƒ
    for (int y = 0; y < 16; y++) {{
        for (int x = 0; x < 16; x++) {{
            float val = pressure_data[y * 16 + x];
            total_pressure += val;
            center_x += x * val;
            center_y += y * val;
        }}
    }}
    
    if (total_pressure > 0) {{
        center_x /= total_pressure;
        center_y /= total_pressure;
    }} else {{
        center_x = center_y = 8.0f;
    }}
    
    features[0] = center_x;
    features[1] = center_y;
    
    // åŒºåŸŸå‹åŠ›åˆ†å¸ƒ
    float left_pressure = 0, right_pressure = 0;
    float top_pressure = 0, bottom_pressure = 0;
    
    for (int y = 0; y < 16; y++) {{
        for (int x = 0; x < 16; x++) {{
            float val = pressure_data[y * 16 + x];
            if (x < 8) left_pressure += val;
            else right_pressure += val;
            if (y < 8) top_pressure += val;
            else bottom_pressure += val;
        }}
    }}
    
    if (total_pressure > 0) {{
        features[2] = left_pressure / total_pressure;
        features[3] = right_pressure / total_pressure;
        features[4] = top_pressure / total_pressure;
        features[5] = bottom_pressure / total_pressure;
    }} else {{
        features[2] = features[3] = features[4] = features[5] = 0.25f;
    }}
    
    // å¯¹ç§°æ€§ç‰¹å¾
    features[6] = left_pressure / (right_pressure + 1e-6f);
    features[7] = top_pressure / (bottom_pressure + 1e-6f);
}}

// æå–å³°å€¼ç‰¹å¾  
void extract_peak_features(const uint16_t* pressure_data, float* features) {{
    // ç®€åŒ–çš„å³°å€¼ç‰¹å¾æå–
    uint16_t max_val = 0;
    float sum = 0;
    
    for (int i = 0; i < 256; i++) {{
        if (pressure_data[i] > max_val) max_val = pressure_data[i];
        sum += pressure_data[i];
    }}
    
    float mean = sum / 256.0f;
    
    features[0] = max_val;                    // æœ€å¤§å€¼
    features[1] = max_val;                    // top5å¹³å‡ï¼ˆç®€åŒ–ä¸ºæœ€å¤§å€¼ï¼‰
    features[2] = max_val / (mean + 1e-6f);   // å³°å€¼ä¸å‡å€¼æ¯”
    features[3] = 1.0f;                       // ç®€åŒ–ä¸º1
}}

// æå–æ‰€æœ‰ç‰¹å¾
void extract_all_features(const uint16_t* pressure_data, float* features) {{
    extract_statistical_features(pressure_data, features);
    extract_spatial_features(pressure_data, features + 8);
    extract_peak_features(pressure_data, features + 16);
}}

// åˆ†ç±»å‡½æ•°
class_t classify_pressure_data(const uint16_t* pressure_data) {{
    float features[20];  // æ€»å…±20ä¸ªç‰¹å¾
    extract_all_features(pressure_data, features);
    
    // éå†å†³ç­–æ ‘
    int node_id = 0;  // ä»æ ¹èŠ‚ç‚¹å¼€å§‹
    
    while (tree_nodes[node_id].feature != -1) {{  // ä¸æ˜¯å¶å­èŠ‚ç‚¹
        int feature_idx = tree_nodes[node_id].feature;
        float threshold = tree_nodes[node_id].threshold;
        
        if (features[feature_idx] <= threshold) {{
            node_id = tree_nodes[node_id].left_child;
        }} else {{
            node_id = tree_nodes[node_id].right_child;
        }}
    }}
    
    return (class_t)tree_nodes[node_id].class_id;
}}

// å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹
int predict_with_confidence(const uint16_t* pressure_data, float* confidence) {{
    class_t result = classify_pressure_data(pressure_data);
    
    // ç®€åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—ï¼ˆåŸºäºæ•°æ®è´¨é‡ï¼‰
    float sum = 0;
    for (int i = 0; i < 256; i++) {{
        sum += pressure_data[i];
    }}
    
    if (sum > 10000) {{
        *confidence = 0.9f;  // é«˜è´¨é‡æ•°æ®
    }} else if (sum > 1000) {{
        *confidence = 0.7f;  // ä¸­ç­‰è´¨é‡æ•°æ®
    }} else {{
        *confidence = 0.5f;  // ä½è´¨é‡æ•°æ®
    }}
    
    return result;
}}
"""
        
        # ä¿å­˜æ–‡ä»¶
        with open(f"{output_dir}/pressure_classifier.h", 'w') as f:
            f.write(header_content)
        
        with open(f"{output_dir}/pressure_classifier.c", 'w') as f:
            f.write(c_content)
        
        print(f"âœ… Cä»£ç å·²ç”Ÿæˆ:")
        print(f"   - {output_dir}/pressure_classifier.h")
        print(f"   - {output_dir}/pressure_classifier.c")
        print(f"   - æ ‘èŠ‚ç‚¹æ•°é‡: {tree.node_count}")
        print(f"   - ç‰¹å¾æ•°é‡: {tree.n_features}")
    
    def save_models(self, output_dir='../embedded/'):
        """ä¿å­˜æ¨¡å‹æ–‡ä»¶"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜sklearnæ¨¡å‹
        for name, model in self.models.items():
            model_file = f"{output_dir}/{name}_model.pkl"
            joblib.dump(model, model_file)
            print(f"âœ… å·²ä¿å­˜: {model_file}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        if self.scalers:
            scaler_file = f"{output_dir}/scaler.pkl"
            joblib.dump(self.scalers['standard'], scaler_file)
            print(f"âœ… å·²ä¿å­˜: {scaler_file}")
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        if self.label_encoder:
            le_file = f"{output_dir}/label_encoder.pkl"
            joblib.dump(self.label_encoder, le_file)
            print(f"âœ… å·²ä¿å­˜: {le_file}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ğŸ”§ STM32åµŒå…¥å¼æ¨¡å‹è®¾è®¡å·¥å…·")
        print("=" * 50)
        print("ğŸ“– ä½¿ç”¨æ–¹æ³•:")
        print("   python stm32_model_designer.py train     - è®­ç»ƒè½»é‡çº§æ¨¡å‹")
        print("   python stm32_model_designer.py generate  - ç”ŸæˆCä»£ç ")
        print("   python stm32_model_designer.py test      - æµ‹è¯•æ¨¡å‹")
        return
    
    command = sys.argv[1]
    designer = STM32ModelDesigner()
    
    if command == 'train':
        print("ğŸš€ è®­ç»ƒSTM32åµŒå…¥å¼æ¨¡å‹")
        
        # åŠ è½½æ•°æ®
        features, labels = designer.load_data('../data/dataset.csv')
        
        # è®­ç»ƒæ¨¡å‹
        results = designer.train_lightweight_models(features, labels)
        
        # ä¿å­˜æ¨¡å‹
        designer.save_models()
        
        # æ˜¾ç¤ºæ€»ç»“
        print(f"\nğŸ¯ è®­ç»ƒæ€»ç»“:")
        print(f"{'æ¨¡å‹åç§°':<20} {'å‡†ç¡®ç‡':<10} {'å¤§å°(KB)':<10} {'é¢„æµ‹æ—¶é—´(ms)':<15}")
        print("-" * 55)
        for name, result in results.items():
            print(f"{name:<20} {result['accuracy']:<10.3f} {result['model_size_kb']:<10.1f} {result['predict_time']*1000:<15.2f}")
        
        # æ¨èæœ€ä½³æ¨¡å‹
        best_model = max(results.items(), key=lambda x: x[1]['accuracy'])
        print(f"\nğŸŒŸ æ¨èæ¨¡å‹: {best_model[0]} (å‡†ç¡®ç‡: {best_model[1]['accuracy']:.3f})")
        
    elif command == 'generate':
        print("ğŸ”§ ç”ŸæˆCä»£ç ")
        
        # å…ˆå°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        try:
            designer.models['decision_tree'] = joblib.load('../embedded/decision_tree_model.pkl')
            designer.generate_c_code('decision_tree')
        except:
            print("âŒ è¯·å…ˆè¿è¡Œè®­ç»ƒ: python stm32_model_designer.py train")
    
    elif command == 'test':
        if len(sys.argv) < 3:
            print("âŒ ç”¨æ³•: python stm32_model_designer.py test <æµ‹è¯•æ•°æ®æ–‡ä»¶> [æœŸæœ›æ ‡ç­¾]")
            sys.exit(1)
        
        test_file = sys.argv[2]
        expected_label = sys.argv[3] if len(sys.argv) > 3 else None
        
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹ - æ•°æ®æ–‡ä»¶: {test_file}")
        if expected_label:
            print(f"   æœŸæœ›æ ‡ç­¾: {expected_label}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_features, test_labels = designer.load_test_data(test_file)
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(test_features)} ä¸ªæ ·æœ¬")
        
        # å¦‚æœæ²¡æœ‰çœŸå®æ ‡ç­¾ä½†æä¾›äº†æœŸæœ›æ ‡ç­¾ï¼Œåˆ›å»ºæ ‡ç­¾æ•°ç»„
        if test_labels is None and expected_label:
            test_labels = [expected_label] * len(test_features)
            print(f"   ä½¿ç”¨æœŸæœ›æ ‡ç­¾ '{expected_label}' ä½œä¸ºçœŸå®æ ‡ç­¾")
        
        # æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        results = {}
        
        # 1. å°è¯•åŠ è½½å¹¶æµ‹è¯•å†³ç­–æ ‘æ¨¡å‹
        try:
            decision_tree = joblib.load('../embedded/decision_tree_model.pkl')
            label_encoder = joblib.load('../embedded/label_encoder.pkl')
            
            predictions = decision_tree.predict(test_features)
            
            if test_labels is not None:
                # ç¼–ç çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ
                encoded_test_labels = label_encoder.transform(test_labels)
                accuracy = accuracy_score(encoded_test_labels, predictions)
                results['å†³ç­–æ ‘'] = accuracy
                print(f"âœ… å†³ç­–æ ‘æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.3f}")
            else:
                print(f"âœ… å†³ç­–æ ‘æ¨¡å‹é¢„æµ‹ç»“æœ:")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            from collections import Counter
            # è§£ç é¢„æµ‹ç»“æœä»¥ä¾¿æ˜¾ç¤º
            decoded_predictions = label_encoder.inverse_transform(predictions)
            pred_counts = Counter(decoded_predictions)
            print(f"   é¢„æµ‹åˆ†å¸ƒ: {dict(pred_counts)}")
            
            if test_labels is not None:
                true_counts = Counter(test_labels)
                print(f"   çœŸå®åˆ†å¸ƒ: {dict(true_counts)}")
            
        except Exception as e:
            print(f"âŒ å†³ç­–æ ‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # 2. å°è¯•åŠ è½½å¹¶æµ‹è¯•ç»Ÿè®¡ç‰¹å¾æ¨¡å‹  
        try:
            # é¦–å…ˆéœ€è¦åŠ è½½æ ‡ç­¾ç¼–ç å™¨å’Œç¼©æ”¾å™¨
            label_encoder = joblib.load('../embedded/label_encoder.pkl')
            scaler = joblib.load('../embedded/scaler.pkl')
            
            # æŸ¥æ‰¾ç»Ÿè®¡ç‰¹å¾æ¨¡å‹
            import os
            stat_models = [f for f in os.listdir('../embedded') if 'model.pkl' in f and f != 'decision_tree_model.pkl']
            
            for model_file in stat_models:
                try:
                    model_name = model_file.replace('_model.pkl', '').replace('_', ' ')
                    stat_model = joblib.load(f'../embedded/{model_file}')
                    
                    # æ ‡å‡†åŒ–ç‰¹å¾
                    test_features_scaled = scaler.transform(test_features)
                    predictions = stat_model.predict(test_features_scaled)
                    
                    if test_labels is not None:
                        # ç¼–ç çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ
                        encoded_test_labels = label_encoder.transform(test_labels)
                        accuracy = accuracy_score(encoded_test_labels, predictions)
                        results[model_name] = accuracy
                        print(f"âœ… {model_name}æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.3f}")
                    else:
                        print(f"âœ… {model_name}æ¨¡å‹é¢„æµ‹ç»“æœ:")
                        # è§£ç é¢„æµ‹ç»“æœ
                        decoded_predictions = label_encoder.inverse_transform(predictions)
                        pred_counts = Counter(decoded_predictions)
                        print(f"   é¢„æµ‹åˆ†å¸ƒ: {dict(pred_counts)}")
                    
                except Exception as e:
                    print(f"âŒ {model_name}æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"âŒ ç»Ÿè®¡ç‰¹å¾æ¨¡å‹ç›¸å…³æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        # 3. å¦‚æœæœ‰åŸå§‹CNNæ¨¡å‹ï¼Œä¹Ÿè¿›è¡Œå¯¹æ¯”æµ‹è¯•
        try:
            import tensorflow as tf
            cnn_model = tf.keras.models.load_model('../models/cnn_augmented_model.keras')
            
            # é‡å¡‘æ•°æ®ä»¥é€‚åº”CNNè¾“å…¥æ ¼å¼ (åŸå§‹256ç»´å‹åŠ›æ•°æ®)
            # éœ€è¦ä½¿ç”¨åŸå§‹å‹åŠ›æ•°æ®è€Œä¸æ˜¯æå–çš„ç‰¹å¾
            df_raw = pd.read_csv(test_file)
            raw_data = df_raw.values  # åŸå§‹256ç»´æ•°æ®
            test_features_cnn = raw_data.reshape(-1, 256, 1)
            
            predictions = cnn_model.predict(test_features_cnn, verbose=0)
            predictions = (predictions > 0.5).astype(int).flatten()
            
            if test_labels is not None:
                # CNNæœŸæœ›çš„æ˜¯äºŒè¿›åˆ¶æ ‡ç­¾
                binary_test_labels = [1 if label == 'crossleg' else 0 for label in test_labels]
                accuracy = accuracy_score(binary_test_labels, predictions)
                results['åŸå§‹CNN'] = accuracy
                print(f"âœ… åŸå§‹CNNæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.3f}")
            else:
                print(f"âœ… åŸå§‹CNNæ¨¡å‹é¢„æµ‹ç»“æœ:")
                pred_labels = ['crossleg' if p == 1 else 'normal' for p in predictions]
                pred_counts = Counter(pred_labels)
                print(f"   é¢„æµ‹åˆ†å¸ƒ: {dict(pred_counts)}")
            
        except Exception as e:
            print(f"âŒ CNNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        if results:
            print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
            print(f"{'æ¨¡å‹ç±»å‹':<15} {'å‡†ç¡®ç‡':<10}")
            print("-" * 25)
            for model_name, accuracy in results.items():
                print(f"{model_name:<15} {accuracy:<10.3f}")
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model = max(results.items(), key=lambda x: x[1])
            print(f"\nğŸŒŸ æœ€ä½³æ¨¡å‹: {best_model[0]} (å‡†ç¡®ç‡: {best_model[1]:.3f})")
            
            # å¦‚æœå‡†ç¡®ç‡è¾ƒä½ï¼Œç»™å‡ºå»ºè®®
            if best_model[1] < 0.8:
                print(f"\nâš ï¸  å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®:")
                print(f"   1. æ£€æŸ¥æµ‹è¯•æ•°æ®è´¨é‡")
                print(f"   2. å¢åŠ è®­ç»ƒæ•°æ®")
                print(f"   3. è°ƒæ•´æ¨¡å‹å‚æ•°")
        else:
            print("\nâŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
    
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")

if __name__ == "__main__":
    main()